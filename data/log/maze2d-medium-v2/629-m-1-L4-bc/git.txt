1f9712464f57c42c0d02bb616730fa3483897abd
diff --git a/README.md b/README.md
index f655020..8f3297c 100644
--- a/README.md
+++ b/README.md
@@ -1,15 +1,15 @@
 # Diffusion Model-Augmented Behavioral Cloning
 
-[Hsiang-Chun Wang\*](https://openreview.net/profile?id=~Hsiang-Chun_Wang1),
 [Shang-Fu Chen\*](https://openreview.net/profile?id=~Shang-Fu_Chen2),
+[Hsiang-Chun Wang\*](https://openreview.net/profile?id=~Hsiang-Chun_Wang1),
 [Ming-Hao Hsu](https://qaz159qaz159.github.io/),
 [Chun-Mao Lai](https://www.mecoli.net/),
-[Shao-Hua Sun](https://shaohua0116.github.io) at [NTU RLL lab](https://github.com/NTURobotLearningLab/)
+[Shao-Hua Sun](https://shaohua0116.github.io) at [NTU RLL lab](https://nturll.xyz/about)
 
-[[Project website]](https://github.com/NTURobotLearningLab/dbc) [[Paper]](https://arxiv.org/abs/2302.13335)
+[[Project website]](https://nturobotlearninglab.github.io/dbc/) [[Paper]](https://arxiv.org/abs/2302.13335)
 
 <!-- TODO: Update publication list -->
-This is the official PyTorch implementation of the paper ["Diffusion Model-Augmented Behavioral Cloning"](https://nturobotlearninglab.github.io/dbc/) (ICML2023 workshop).
+This is the official PyTorch implementation of the paper ["Diffusion Model-Augmented Behavioral Cloning"](https://nturobotlearninglab.github.io/dbc/) (ICML2024).
 
 ![image](docs/img/framework.jpeg)
 
diff --git a/configs/maze/bc.yaml b/configs/maze/bc.yaml
index d587c4a..647aa19 100644
--- a/configs/maze/bc.yaml
+++ b/configs/maze/bc.yaml
@@ -45,15 +45,13 @@ parameters:
   save-interval:
     value: 20000 
   traj-load-path:
-    value: ./expert_datasets/maze2d_100.pt 
+    value: ./expert_datasets/maze.pt 
   vid-fps:
     value: 60 
   log-diff-loss:
     value: True 
   prefix:
-    value: bc-100 
-  ddpm-path:
-    value: ./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt 
+    value: bc
   bc-state-norm:
     value: True 
   il-in-action-norm:
diff --git a/dbc/main.py b/dbc/main.py
index d0df5c0..2aa39fc 100644
--- a/dbc/main.py
+++ b/dbc/main.py
@@ -11,8 +11,6 @@ from rlf import run_policy, evaluate_policy
 from rlf.algos import (PPO, BaseAlgo, BehavioralCloning, DBC, 
                        DiffPolicy, Ae_bc, Eng_bc, GANBC, IBC)
 from rlf.algos.il.base_il import BaseILAlgo
-from rlf.algos.il.gaifo import GAIFO
-from rlf.algos.il.sqil import SQIL
 from rlf.algos.nested_algo import NestedAlgo
 from rlf.algos.off_policy.sac import SAC
 from rlf.args import str2bool
@@ -39,11 +37,11 @@ import dbc.envs.gridworld
 import dbc.envs.hand
 import dbc.gym_minigrid
 from dbc.envs.goal_traj_saver import GoalTrajSaver
-from dbc.method.airl import ProxAirl
-from dbc.method.discounted_pf import DiscountedProxFunc, DiscountedProxIL
-from dbc.method.goal_gail_discriminator import GoalGAIL
-from dbc.method.ranked_pf import RankedProxIL
-from dbc.method.uncert_discrim import UncertGAIL
+# from dbc.method.airl import ProxAirl
+# from dbc.method.discounted_pf import DiscountedProxFunc, DiscountedProxIL
+# from dbc.method.goal_gail_discriminator import GoalGAIL
+# from dbc.method.ranked_pf import RankedProxIL
+# from dbc.method.uncert_discrim import UncertGAIL
 from dbc.method.utils import trim_episodes_trans
 from dbc.models import GwImgEncoder
 from dbc.policies.grid_world_expert import GridWorldExpert
@@ -282,8 +280,6 @@ class GoalProxSettings(RunSettings):
         algo = get_setup_dict()[self.base_args.alg][0]
         if isinstance(algo, NestedAlgo) and isinstance(algo.modules[0], BaseILAlgo):
             algo.modules[0].set_transform_dem_dataset_fn(trim_episodes_trans)
-        if isinstance(algo, SQIL):
-            algo.il_algo.set_transform_dem_dataset_fn(trim_episodes_trans)
         return algo
 
     def get_logger(self):
@@ -304,9 +300,9 @@ class GoalProxSettings(RunSettings):
         parser.add_argument("--depth", type=int, default=2)
         parser.add_argument("--weight-decay", type=float, default=0.0)
         parser.add_argument("--stochastic-optimizer-train_samples", type=int, default=64)
-        parser.add_argument("--dp-scheduler-type", type=str, default='linear')
         parser.add_argument("--rnd-id", type=str, default='default')
         parser.add_argument("--collect-all", type=str2bool, default=False)
+        parser.add_argument("--bc-state-norm", type=str2bool, default=True)
         parser.add_argument("--agent_expert_normalization", type=str2bool, default=True)
 
     def import_add(self):
diff --git a/dbc/test.py b/dbc/test.py
deleted file mode 100644
index 5c95005..0000000
--- a/dbc/test.py
+++ /dev/null
@@ -1,193 +0,0 @@
-import sys, os
-
-sys.path.insert(0, "./")
-
-from functools import partial
-
-import d4rl
-import torch.nn as nn
-from rlf import run_policy, evaluate_policy
-from rlf.algos import (GAIL, Diff_gail, PPO, BaseAlgo, BehavioralCloning, Diff_bc,
-                       BehavioralCloningFromObs, BehavioralCloningPretrain,
-                       GailDiscrim, Ae_bc)
-from rlf.algos.il.base_il import BaseILAlgo
-from rlf.algos.il.gaifo import GAIFO
-from rlf.algos.il.sqil import SQIL
-from rlf.algos.nested_algo import NestedAlgo
-from rlf.algos.off_policy.sac import SAC
-from rlf.args import str2bool
-from rlf.policies import BasicPolicy, DistActorCritic, RandomPolicy
-from rlf.policies.action_replay_policy import ActionReplayPolicy
-from rlf.policies.actor_critic.dist_actor_q import (DistActorQ, get_sac_actor,
-                                                    get_sac_critic)
-from rlf.policies.actor_critic.reg_actor_critic import RegActorCritic
-from rlf.rl.loggers.base_logger import BaseLogger
-from rlf.rl.loggers.wb_logger import (WbLogger, get_wb_ray_config,
-                                      get_wb_ray_kwargs)
-from rlf.rl.model import CNNBase, MLPBase, MLPBasic, TwoLayerMlpWithAction
-from rlf.run_settings import RunSettings
-
-import dbc.envs.ball_in_cup
-import dbc.envs.d4rl
-import dbc.envs.fetch
-import dbc.envs.goal_check
-import dbc.envs.gridworld
-import dbc.envs.hand
-import dbc.gym_minigrid
-from dbc.envs.goal_traj_saver import GoalTrajSaver
-from dbc.method.airl import ProxAirl
-from dbc.method.discounted_pf import DiscountedProxFunc, DiscountedProxIL
-from dbc.method.goal_gail_discriminator import GoalGAIL
-from dbc.method.ranked_pf import RankedProxIL
-from dbc.method.uncert_discrim import UncertGAIL
-from dbc.method.utils import trim_episodes_trans
-from dbc.models import GwImgEncoder
-from dbc.policies.grid_world_expert import GridWorldExpert
-
-import time
-
-os.environ["CUDA_VISIBLE_DEVICES"] = "1"
-
-def get_ppo_policy(env_name, args):
-    if env_name.startswith("MiniGrid") and args.gw_img:
-        return DistActorCritic(get_base_net_fn=lambda i_shape: GwImgEncoder(i_shape))
-
-    return DistActorCritic()
-
-
-def get_deep_ppo_policy(env_name, args):
-    return DistActorCritic(
-        get_actor_fn=lambda _, i_shape: MLPBasic(
-            i_shape[0], hidden_size=256, num_layers=2
-        ),
-        get_critic_fn=lambda _, i_shape, asp: MLPBasic(
-            i_shape[0], hidden_size=256, num_layers=2
-        ),
-    )
-
-
-def get_deep_sac_policy(env_name, args):
-    return DistActorQ(
-        get_critic_fn=partial(get_sac_critic, hidden_dim=256),
-        get_actor_fn=partial(get_sac_actor, hidden_dim=256),
-    )
-
-
-def get_deep_ddpg_policy(env_name, args):
-    def get_actor_head(hidden_dim, action_dim):
-        return nn.Sequential(nn.Linear(hidden_dim, action_dim), nn.Tanh())
-
-    return RegActorCritic(
-        get_actor_fn=lambda _, i_shape: MLPBase(i_shape[0], False, (256, 256)),
-        get_actor_head_fn=get_actor_head,
-        get_critic_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-            i_shape[0], (256, 256), a_space.shape[0]
-        ),
-    )
-
-
-def get_basic_policy(env_name, args, is_stoch):
-    if env_name.startswith("MiniGrid") and args.gw_img:
-        return BasicPolicy(
-            is_stoch=is_stoch, get_base_net_fn=lambda i_shape: GwImgEncoder(i_shape)
-        )
-    else:
-        return BasicPolicy(
-            is_stoch=is_stoch,
-            get_base_net_fn=lambda i_shape: MLPBasic(
-                i_shape[0], hidden_size=256, num_layers=2
-            ),
-        )
-
-    return BasicPolicy()
-
-
-def get_deep_basic_policy(env_name, args):
-    return BasicPolicy(
-        get_base_net_fn=lambda i_shape: MLPBase(i_shape[0], False, (512, 512, 256, 128))
-    )
-
-
-def get_setup_dict():
-    return {
-        "gail": (GAIL(), get_ppo_policy),
-        "gail-deep": (GAIL(), get_deep_ppo_policy),
-        "diff-gail": (Diff_gail(), get_deep_ppo_policy),
-        "uncert-gail-deep": (UncertGAIL(), get_deep_ppo_policy),
-        "uncert-gail": (UncertGAIL(), get_ppo_policy),
-        "gaifo": (GAIFO(), get_ppo_policy),
-        "gaifo-deep": (GAIFO(), get_deep_ppo_policy),
-        "ppo": (PPO(), get_ppo_policy),
-        "ppo-deep": (PPO(), get_deep_ppo_policy),
-        "gw-exp": (BaseAlgo(), lambda env_name, _: GridWorldExpert()),
-        "action-replay": (BaseAlgo(), lambda env_name, _: ActionReplayPolicy()),
-        "rnd": (BaseAlgo(), lambda env_name, _: RandomPolicy()),
-        "bc": (BehavioralCloning(), partial(get_basic_policy, is_stoch=False)),
-        "diff-bc": (Diff_bc(), partial(get_basic_policy, is_stoch=False)),
-        "ae-bc": (Ae_bc(), partial(get_basic_policy, is_stoch=False)),
-        "bco": (BehavioralCloningFromObs(), partial(get_basic_policy, is_stoch=True)),
-        "bc-deep": (BehavioralCloning(), get_deep_basic_policy),
-        "dpf": (DiscountedProxIL(), get_ppo_policy),
-        "dpf-deep": (DiscountedProxIL(), get_deep_ppo_policy),
-        "dpf-deep-im": (
-            DiscountedProxIL(
-                get_pf_base=lambda i_shape: CNNBase(i_shape[0], False, 256),
-            ),
-            get_deep_ppo_policy,
-        ),
-        "prox-deep": (ProxAirl(), get_deep_ppo_policy),
-        "rpf": (RankedProxIL(), get_ppo_policy),
-        "rpf-deep": (RankedProxIL(), get_deep_ppo_policy),
-        "sqil-deep": (SQIL(), get_deep_sac_policy),
-        "sac": (SAC(), get_deep_sac_policy),
-        "goal-gail": (GoalGAIL(), get_deep_ddpg_policy),
-    }
-
-
-class GoalProxSettings(RunSettings):
-    def get_policy(self):
-        return get_setup_dict()[self.base_args.alg][1](
-            self.base_args.env_name, self.base_args
-        )
-
-    def create_traj_saver(self, save_path):
-        return GoalTrajSaver(save_path, False)
-
-    def get_algo(self):
-        algo = get_setup_dict()[self.base_args.alg][0]
-        if isinstance(algo, NestedAlgo) and isinstance(algo.modules[0], BaseILAlgo):
-            algo.modules[0].set_transform_dem_dataset_fn(trim_episodes_trans)
-        if isinstance(algo, SQIL):
-            algo.il_algo.set_transform_dem_dataset_fn(trim_episodes_trans)
-        return algo
-
-    def get_logger(self):
-        if self.base_args.no_wb:
-            return BaseLogger()
-        else:
-            return WbLogger()
-
-    def get_add_args(self, parser):
-        parser.add_argument("--alg")
-        parser.add_argument("--env-name")
-        # Should always be true!
-        parser.add_argument("--gw-img", type=str2bool, default=True)
-        parser.add_argument("--no-wb", action="store_true", default=False)
-
-    def import_add(self):
-        import dbc.envs.fetch
-        import dbc.envs.goal_check
-
-    def get_add_ray_config(self, config):
-        if self.base_args.no_wb:
-            return config
-        return get_wb_ray_config(config)
-
-    def get_add_ray_kwargs(self):
-        if self.base_args.no_wb:
-            return {}
-        return get_wb_ray_kwargs()
-
-
-if __name__ == "__main__":
-    evaluate_policy(GoalProxSettings())
diff --git a/requirements.txt b/requirements.txt
index 5d1b062..a693789 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -79,3 +79,4 @@ dcargs
 omegaconf==2.3.0
 hydra-core==1.3.2
 scikit-learn
+patchelf
\ No newline at end of file
diff --git a/rl-toolkit/README.md b/rl-toolkit/README.md
deleted file mode 100644
index afffc15..0000000
--- a/rl-toolkit/README.md
+++ /dev/null
@@ -1,124 +0,0 @@
-# RL Toolkit (RLT)
-
-Codebase I use to to implement RL algorithms.
-
-## Algorithms
-- On policy:
-  - REINFORCE
-  - Actor Critic (A2C)
-  - Proximal Policy Optimization (PPO)
-- Off policy:
-  - Deep Q-Networks (DQN)
-  - Deep Deterministic Policy Gradients (DDPG)
-  - Soft Actor Critic (SAC) 
-  - Hindsight Experience Replay (HER)
-- Hierarchical RL: 
-  - Option Critic
-- Imitation learning:
-  - Behavioral Cloning (BC)
-  - Generative Adversarial Imitation Learning (GAIL)
-- Imitation learning from observation: 
-  - Generative Adversarial Imitation from Observation (GAIfO)
-  - Behavioral Cloning from Observations (BCO)
-
-See learning curves for these algorithms [below](https://github.com/ASzot/rl-toolkit#benchmarks)
-
-## Documentation
-- Loggers: `rl-toolkit/rlf/rl/loggers/README.md`
-
-## Code Features
-- Custom policies. 
-- Custom update functions.
-- Configurable replay buffer or trajectory storage. Control how you collect
-  agent experience. 
-- Custom loggers. Default integration for TensorBoard and W&B.
-- Define environment wrappers. Use this to log custom environment statistics,
-  define and pass command line arguments, and add wrappers. 
-- Environment multi-processing.
-- Integration with Ray auto hyperparameter tuning. 
-- Automated experiment runner. Includes command templates, multiple seed
-  runner, and tmux integration. 
-- Auto figure creation.
-
-## Installation
-Requires Python 3.7 or higher. With conda: 
-
-- Clone the repo
-- `conda create -n rlf python=3.7`
-- `source activate rlf`
-- `pip install -r requirements.txt`. 
-- `pip install -e .`
-
-If you want to install MuJoCo as well: `mujoco-py==2.0.2.5` 
-
-## Usage
-See `tests/test_cmds/ppo/main.py` for an example of using PPO. Example
-commands for training PPO are in the same folder in the `.cmd` files. See the
-`tests/test_cmds/` for more examples.
-
-## Run Tests
-The most important principle in this code is **working RL algorithms**.
-Automated benchmarking scripts are included under `tests/test_cmds` so you can
-be sure the code is working. For example, to run the PPO benchmark on Hopper-v3
-with 5 seeds, run: `python -m rlf --cfg tests/config.yaml --cmd ppo/hopper  --seed
-"31,41,51,61,71"  --sess-id 0`.
-
-## Experiment Runner
-Easily run templated commands. Start by defining a `.cmd` file. 
-- Send to new tmux pane. 
-- Easily run and manage long complicated commands. 
-- Add additional arguments to specified command. 
-- Specify which GPU to use via a flag. 
-- Choose to log to W&B. 
-
-## Custom Environments
-See `envs/README.md`
-
-## Ray
-Install with `pip install ray` and `pip install "ray[tune]"`. To run a job with
-Ray specify `--ray` and specify your hyperparam search for Ray tune using
-Python syntax in the command line argument with `--ray-config "{'lr':
-tune.uniform(0.01, 0.001)}"`. You can specify additional settings such as
-`--ray-cpus`, `--ray-gpus`, `--ray-nsamples`. `--ray-debug` runs Ray in serial
-mode. When using Ray you cannot reference global variables from anywhere in
-your RunSettings.
-
-# Benchmarks
-### Hopper-v3
-
-Commit: `570d8c8d024cb86266610e72c5431ef17253c067`
-- PPO: `python -m rlf --cmd ppo/hopper --cd 0 --cfg ./tests/config.yaml --seed "31,41,51" --sess-id 0 --cuda False` 
-
-![Hopper-v3](https://github.com/ASzot/rl-toolkit/blob/master/bench_plots/hopper.png)
-
-### HalfCheetah-v3
-Commit: `58644db1ac638ba6c8a22e7a01eacfedffd4a49f`
-- PPO: `python -m rlf --cmd ppo/halfcheetah --cd 0 --cfg ./tests/config.yaml --seed "31,41,51" --sess-id 0 --cuda False`
-
-![Hopper-v3](https://github.com/ASzot/rl-toolkit/blob/master/bench_plots/halfcheetah.png)
-
-### HalfCheetah-v3 Imitation Learning
-Commit: `58644db1ac638ba6c8a22e7a01eacfedffd4a49f`
-- BCO: `python -m rlf --cmd bco/halfcheetah --cfg ./tests/config.yaml --seed "31,41,51" --sess-id 0 --cuda False` 
-- GAIfO-s: `python -m rlf --cmd gaifo_s/halfcheetah --cfg ./tests/config.yaml --seed "31,41,51" --sess-id 0 --cuda False` 
-- GAIfO: `python -m rlf --cmd gaifo/halfcheetah --cfg ./tests/config.yaml --seed "31,41,51" --sess-id 0 --cuda False` 
-
-![Hopper-v3](https://github.com/ASzot/rl-toolkit/blob/master/bench_plots/halfcheetah_il.png)
-
-### Pendulum-v0
-Commit: `5c051769088b6582b0b31db9a145738a9ed68565`
-- DDPG: `python -m rlf --cmd ddpg/pendulum --cd 0 --cfg ./tests/config.yaml --seed "31,41" --sess-id 0 --cuda False`
-
-![Pendulum-v0](https://github.com/ASzot/rl-toolkit/blob/master/bench_plots/pendulum.png)
-
-### HER
-Commit: `95bb3a7d0bf1945e414a0e77de8a749bd79dc554`
-- BitFlip: `python -m rlf --cmd her/bit_flip --cfg ./tests/config.yaml --cuda False --sess-id 0`
-
-![HER](https://github.com/ASzot/rl-toolkit/blob/master/bench_plots/her.png)
-
-# Sources
-* The SAC code is a clone of https://github.com/denisyarats/pytorch_sac.
-  The license is at `rlf/algos/off_policy/denis_yarats_LICENSE.md`
-* The PPO and rollout storage code is based on https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail.
-* The environment preprocessing uses a stripped down version of https://github.com/openai/baselines.
diff --git a/rl-toolkit/checkmodel.py b/rl-toolkit/checkmodel.py
deleted file mode 100644
index 48b7f77..0000000
--- a/rl-toolkit/checkmodel.py
+++ /dev/null
@@ -1,11 +0,0 @@
-import torch
-
-# Load the model
-model = torch.load('/home/mhhsu/dbc-private/rl-toolkit/walker_5traj_processed_ddpm_1e-5.pt')
-
-# Print the model's structure
-print(model)
-
-# Print the shape of the model's weights
-for param in model.parameters():
-    print(param.size())
\ No newline at end of file
diff --git a/rl-toolkit/examples/.gitignore b/rl-toolkit/examples/.gitignore
deleted file mode 100644
index 74eb56e..0000000
--- a/rl-toolkit/examples/.gitignore
+++ /dev/null
@@ -1 +0,0 @@
-test_envs
diff --git a/rl-toolkit/examples/README.md b/rl-toolkit/examples/README.md
deleted file mode 100644
index dd3399a..0000000
--- a/rl-toolkit/examples/README.md
+++ /dev/null
@@ -1,14 +0,0 @@
-All plotted results are under the root README.
-
-Example of running a command: `python -m rlf --cmd ddpg/pendulum --cd 0 --cfg ./tests/config.yaml --seed "31,41" --sess-id 0 --cuda False`
-
-# Expert Datasets
-All under `tests/expert_demonstrations`
-* `halfcheetah_50ep.pt` 50 episodes (1k transitions each), 6678 average reward
-
-# Directory Structure
-- `dev` are under development and for my personal use `--cfg ./tests/config_dev.yaml`. 
-- `test_cmds` are confirmed to work. To run commands specify `--cfg ./tests/config.yaml`. 
-
-# Sanity Checking Algorithms
-- Discrete: CartPole-v0 and get above 195.0 reward for over 100 episodes.
diff --git a/rl-toolkit/examples/__init__.py b/rl-toolkit/examples/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/rl-toolkit/examples/config.yaml b/rl-toolkit/examples/config.yaml
deleted file mode 100644
index 0194f5d..0000000
--- a/rl-toolkit/examples/config.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-  "conda_env": "rlf",
-  "cmds_loc": "examples/test_cmds/",
-  "wb_entity": "aszot",
-  "proj_name": "andrew-random",
-}
diff --git a/rl-toolkit/examples/config_dev.yaml b/rl-toolkit/examples/config_dev.yaml
deleted file mode 100644
index 00c4043..0000000
--- a/rl-toolkit/examples/config_dev.yaml
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-  "conda_env": "rlf",
-  "cmds_loc": "examples/dev/",
-  "proj_name": "andrew-random",
-  "base_data_dir": "/srv/share/aszot3/p-viz-plan",
-}
diff --git a/rl-toolkit/examples/dev/a2c_atari.cmd b/rl-toolkit/examples/dev/a2c_atari.cmd
deleted file mode 100644
index b359770..0000000
--- a/rl-toolkit/examples/dev/a2c_atari.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run_a2c.py --num-env-steps 1e7 --save-interval -1 --eval-interval -1 --linear-lr-deca False --num-processes 32 --num-steps 3 --lr 7e-4 --use-gae False --prefix 'a2c'
diff --git a/rl-toolkit/examples/dev/bandits/README.md b/rl-toolkit/examples/dev/bandits/README.md
deleted file mode 100644
index 3505b8f..0000000
--- a/rl-toolkit/examples/dev/bandits/README.md
+++ /dev/null
@@ -1,2 +0,0 @@
-## Requirements
-
diff --git a/rl-toolkit/examples/dev/bandits/run.py b/rl-toolkit/examples/dev/bandits/run.py
deleted file mode 100644
index dbbe069..0000000
--- a/rl-toolkit/examples/dev/bandits/run.py
+++ /dev/null
@@ -1,16 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from rlf.policies.tabular.action_value_policy import ActionValuePolicy
-from rlf.algos.tabular.bandit_algos import SimpleBanditAlgo
-from tests.test_run_settings import TestRunSettings
-import gym_bandits
-
-class BanditRunSettings(TestRunSettings):
-    def get_policy(self):
-        return ActionValuePolicy()
-
-    def get_algo(self):
-        return SimpleBanditAlgo()
-
-run_policy(BanditRunSettings())
diff --git a/rl-toolkit/examples/dev/bandits/ucb_bandits.cmd b/rl-toolkit/examples/dev/bandits/ucb_bandits.cmd
deleted file mode 100644
index b1905ba..0000000
--- a/rl-toolkit/examples/dev/bandits/ucb_bandits.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/bandits/run.py --eval-interval -1 --save-interval -1 --log-smooth-len 1 --env-name BanditTenArmedGaussian-v0 --num-env-steps 1e3
diff --git a/rl-toolkit/examples/dev/classic_algs/def.py b/rl-toolkit/examples/dev/classic_algs/def.py
deleted file mode 100644
index 49595bd..0000000
--- a/rl-toolkit/examples/dev/classic_algs/def.py
+++ /dev/null
@@ -1,35 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf.algos.tabular.td_methods import TabularTdMethods
-from rlf.algos.tabular.mc_methods import TabularMcMethods
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.tabular.q_table import QTable
-from rlf.rl.loggers.plt_logger import PltLogger
-from rlf.rl.loggers.wb_logger import WbLogger
-
-class ClassicAlgRunSettings(TestRunSettings):
-    def get_policy(self):
-        return QTable()
-
-    def get_algo(self):
-        if self.base_args.alg_type == 'td':
-            return TabularTdMethods()
-        elif self.base_args.alg_type == 'mc':
-            return TabularMcMethods()
-        else:
-            raise ValueError(f"Unrecognized option {self.base_args.alg_type}")
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--alg-type', type=str, default='td')
-        parser.add_argument('--wb', action='store_true', default=False)
-
-    def get_logger(self):
-        if self.base_args.wb:
-            return WbLogger()
-        else:
-            return PltLogger(['avg_r'], '# Updates', ['Reward'], ['Cliff Walker'])
-
-if __name__ == "__main__":
-    run_policy(ClassicAlgRunSettings())
diff --git a/rl-toolkit/examples/dev/classic_algs/mc_blackjack.cmd b/rl-toolkit/examples/dev/classic_algs/mc_blackjack.cmd
deleted file mode 100644
index 6049bb6..0000000
--- a/rl-toolkit/examples/dev/classic_algs/mc_blackjack.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/classic_algs/def.py --prefix 'classic-test' --env-name Blackjack-v0 --log-smooth-len 10  --num-env-steps 1e4 --log-interval 100 --eval-interval -1 --eps-start 0.1 --gamma 1.0 --alg-type 'mc'
diff --git a/rl-toolkit/examples/dev/classic_algs/td_cliff.cmd b/rl-toolkit/examples/dev/classic_algs/td_cliff.cmd
deleted file mode 100644
index 1081db2..0000000
--- a/rl-toolkit/examples/dev/classic_algs/td_cliff.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/classic_algs/def.py --prefix 'classic-test' --env-name CliffWalking-v0 --log-smooth-len 10  --num-env-steps 1e4 --log-interval 100 --eval-interval -1 --eps-start 0.1 --lr 0.5 --gamma 1.0 --alg-type 'td'
diff --git a/rl-toolkit/examples/dev/ddpg/cheetah.cmd b/rl-toolkit/examples/dev/ddpg/cheetah.cmd
deleted file mode 100644
index 6dfeb1d..0000000
--- a/rl-toolkit/examples/dev/ddpg/cheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/ddpg/def.py --prefix 'ddpg-test' --use-proper-time-limits --num-env-steps 3e6 --env-name "HalfCheetah-v2" --eval-interval -1 --log-smooth-len 10 --save-interval -1 --lr 1e-3 --critic-lr 1e-3 --tau 0.005 --n-rnd-steps 10000 --noise-std 0.1 --warmup-steps 1000 --update-every 50 --trans-buffer-size 1000000 --batch-size 100 --linear-lr-decay False --max-grad-norm -1
diff --git a/rl-toolkit/examples/dev/ddpg/def.py b/rl-toolkit/examples/dev/ddpg/def.py
deleted file mode 100644
index 192ee14..0000000
--- a/rl-toolkit/examples/dev/ddpg/def.py
+++ /dev/null
@@ -1,42 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import DDPG
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf import RegActorCritic
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-import torch.nn as nn
-import torch.nn.functional as F
-
-def reg_init(m):
-    return m
-
-def get_actor_head(hidden_dim, action_dim):
-    return nn.Sequential(
-            nn.Linear(hidden_dim, action_dim),
-            nn.Tanh())
-
-class DDPGRunSettings(TestRunSettings):
-    def get_policy(self):
-        if 'Pendulum' in self.base_args.env_name:
-            hidden_size = 128
-        else:
-            hidden_size = 256
-        return RegActorCritic(
-                get_actor_fn=lambda _, i_shape: MLPBase(
-                    i_shape[0], False, (hidden_size, hidden_size),
-                    weight_init=reg_init,
-                    get_activation=lambda: nn.ReLU()),
-                get_actor_head_fn=get_actor_head,
-                get_critic_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                    i_shape[0], (hidden_size, hidden_size), a_space.shape[0],
-                    weight_init=reg_init,
-                    get_activation=lambda: nn.ReLU()),
-                get_critic_head_fn = lambda hidden_dim: nn.Linear(hidden_dim, 1)
-                )
-
-    def get_algo(self):
-        return DDPG()
-
-if __name__ == "__main__":
-    run_policy(DDPGRunSettings())
diff --git a/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper.cmd b/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper.cmd
deleted file mode 100644
index 13e2e25..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --prefix ddpg  --use-proper-time-limits --save-interval -1 --alg ddpg --env-log-dir ~/tmp --trans-buffer-size 1000000 --batch-size 128 --lr 1e-4 --critic-lr 1e-3 --num-env-steps 3000000 --noise-std 0.1 --warmup-steps 0 --noise-type uh --noise-std 0.3 --n-rnd-steps 0 --env-name "Hopper-v3"
diff --git a/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper_2.cmd b/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper_2.cmd
deleted file mode 100644
index 8dbf780..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/ddpg_hopper_2.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --prefix ddpg  --use-proper-time-limits --save-interval -1 --alg ddpg --env-log-dir ~/tmp --trans-buffer-size 1000000 --batch-size 128 --lr 1e-3 --critic-lr 1e-3 --num-env-steps 3000000 --noise-std 0.1 --warmup-steps 0 --noise-type gaussian --noise-std 0.1 --n-rnd-steps 10000 --env-name "Hopper-v3" --tau 5e-3 --warmup-steps 1000 --update-every 50 
diff --git a/rl-toolkit/examples/dev/dev_cmds/gaifo/README.md b/rl-toolkit/examples/dev/dev_cmds/gaifo/README.md
deleted file mode 100644
index d03b51d..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/gaifo/README.md
+++ /dev/null
@@ -1,18 +0,0 @@
-## Hopper Steps
-1. Train expert policy: 
-
-```
-python tests/run_alg.py --prefix ppo-test --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --alg ppo --env-name Hopper-v3 --env-log-dir /home/aszot/tmp --eval-interval -1 --log-smooth-len 10 --cuda False --seed 41
-```
-
-2. Generate 5 demonstration from expert policy. Note that they take so long to generate because the episodes are so long. 
-
-```
-python tests/run_alg.py --prefix ppo-test --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --alg ppo --env-name Hopper-v3 --env-log-dir /home/aszot/tmp --eval-interval -1 --log-smooth-len 10 --cuda False --load-file ./data/trained_models/Hopper-v3/529-H-31-IN-ppo-test/model_899.pt --eval-only --eval-save --eval-num-processes 5 --num-eval 1
---
-```
-
-3. Train GAIfO
-```
-py -m rlf --cfg tests/config.yaml --cmd gaifo/hopper --traj-load-path ./data/traj/Hopper-v3/529-H-31-PQ-ppo-test/trajs.pt
-```
diff --git a/rl-toolkit/examples/dev/dev_cmds/gaifo/hopper.cmd b/rl-toolkit/examples/dev/dev_cmds/gaifo/hopper.cmd
deleted file mode 100644
index 945c3ad..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/gaifo/hopper.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --alg gaifo --prefix gaifo --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --env-name Hopper-v3 --env-log-dir /home/aszot/tmp --eval-interval -1 --log-smooth-len 10 --disc-lr 0.0001
diff --git a/rl-toolkit/examples/dev/dev_cmds/gaifo_cartpole.cmd b/rl-toolkit/examples/dev/dev_cmds/gaifo_cartpole.cmd
deleted file mode 100644
index b7c0a5a..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/gaifo_cartpole.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --alg gaifo --prefix gaifo --env-name CartPole-v1 --eval-num-processes 32 --num-render 0
diff --git a/rl-toolkit/examples/dev/dev_cmds/gail/hopper.cmd b/rl-toolkit/examples/dev/dev_cmds/gail/hopper.cmd
deleted file mode 100644
index 32ec31d..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/gail/hopper.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --alg gail --prefix gail --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --env-name Hopper-v3 --env-log-dir /home/aszot/tmp --eval-interval -1 --log-smooth-len 10 --disc-lr 0.0001 --entropy-coef 0.001
diff --git a/rl-toolkit/examples/dev/dev_cmds/ppo_atari.cmd b/rl-toolkit/examples/dev/dev_cmds/ppo_atari.cmd
deleted file mode 100644
index 5fdd5cc..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/ppo_atari.cmd
+++ /dev/null
@@ -1,2 +0,0 @@
-python tests/run_alg.py --env-name "BreakoutNoFrameskip-v4" --alg ppo --lr 2.5e-4 --clip-param 0.1 --value-loss-coef 0.5 --num-processes 8 --num-steps 128 --num-mini-batch 4 --log-interval 1 --linear-lr-decay True --entropy-coef 0.01 --prefix ppo-test --eval-interval -1 --vid-fps 30.0
-
diff --git a/rl-toolkit/examples/dev/dev_cmds/ray_test.cmd b/rl-toolkit/examples/dev/dev_cmds/ray_test.cmd
deleted file mode 100644
index 09f58ab..0000000
--- a/rl-toolkit/examples/dev/dev_cmds/ray_test.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/tune_test.py --num-env-steps 1e5 --linear-lr-decay True --lr 0.001 --prefix 'ppo-cartpole-test' --num-steps 32 --num-mini-batch 1 --num-epochs 20 --entropy-coef 0.0 --env-name "CartPole-v1" --num-processes 8 --eval-interval -1 --save-interval -1 --log-smooth-len 1 
diff --git a/rl-toolkit/examples/dev/dqn/main.py b/rl-toolkit/examples/dev/dqn/main.py
deleted file mode 100644
index 531f832..0000000
--- a/rl-toolkit/examples/dev/dqn/main.py
+++ /dev/null
@@ -1,58 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import QLearning
-from rlf import run_policy
-from rlf.rl.model import BaseNet
-from tests.test_run_settings import TestRunSettings
-from rlf import RegActorCritic
-from rlf import DQN
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-import torch.nn as nn
-import torch.nn.functional as F
-from rlf.args import str2bool
-
-
-class GwImgEncoder(BaseNet):
-    """
-    Custom image encoder to support the Grid World environment with image
-    observations (rather than flattened observations). This is important for
-    large grids.
-    """
-    def __init__(self, obs_shape, hidden_size=64):
-        super().__init__(False, hidden_size, hidden_size)
-
-        # Network architecture inspired by https://github.com/lcswillems/rl-starter-files/blob/master/model.py
-        n = obs_shape[1]
-        m = obs_shape[2]
-        image_embedding_size = ((n-1)//2-2)*((m-1)//2-2)*64
-
-        self.net = nn.Sequential(
-                nn.Conv2d(obs_shape[0], 16, (2, 2)),
-                nn.ReLU(),
-                nn.MaxPool2d((2, 2)),
-                nn.Conv2d(16, 32, (2, 2)),
-                nn.ReLU(),
-                nn.Conv2d(32, 64, (2, 2)),
-                nn.ReLU(),
-                Flatten(),
-                nn.Linear(image_embedding_size, hidden_size),
-                nn.ReLU(),
-            )
-
-    def forward(self, inputs, rnn_hxs, masks):
-        return self.net(inputs), rnn_hxs
-
-class DqnRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DQN(
-                get_base_net_fn=lambda i_shape, recurrent: GwImgEncoder(i_shape),
-                )
-
-    def get_algo(self):
-        return QLearning()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-
-if __name__ == "__main__":
-    run_policy(DqnRunSettings())
diff --git a/rl-toolkit/examples/dev/her/def.py b/rl-toolkit/examples/dev/her/def.py
deleted file mode 100644
index 65ed973..0000000
--- a/rl-toolkit/examples/dev/her/def.py
+++ /dev/null
@@ -1,65 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import DDPG
-from rlf import QLearning
-from rlf.algos.off_policy.her import HerStorage
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf import RegActorCritic
-from rlf import DQN
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-import torch.nn as nn
-import torch.nn.functional as F
-from rlf.args import str2bool
-
-
-def reg_init(m):
-    return m
-
-def get_actor_head(hidden_dim, action_dim):
-    return nn.Sequential(
-            nn.Linear(hidden_dim, action_dim),
-            nn.Tanh())
-
-class HerRunSettings(TestRunSettings):
-    def get_policy(self):
-        hidden_size = 256
-        if 'BitFlip' in self.base_args.env_name:
-            return DQN(
-                    get_base_net_fn=lambda i_shape, recurrent: MLPBase(
-                        i_shape[0], False, (hidden_size,),
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    use_goal=True
-                    )
-        else:
-            return RegActorCritic(
-                    get_actor_fn=lambda _, i_shape: MLPBase(
-                        i_shape[0], False, (hidden_size, hidden_size),
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    get_actor_head_fn=get_actor_head,
-                    get_critic_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                        i_shape[0], (hidden_size, hidden_size), a_space.shape[0],
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    get_critic_head_fn = lambda hidden_dim: nn.Linear(hidden_dim, 1),
-                    use_goal=True
-                    )
-
-    def get_algo(self):
-        pass_kwargs = {}
-        if self.base_args.use_her:
-            pass_kwargs['get_storage_fn'] = lambda buff_size, args: \
-                HerStorage(buff_size, args)
-        if 'BitFlip' in self.base_args.env_name:
-            return QLearning(**pass_kwargs)
-        else:
-            return DDPG(**pass_kwargs)
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--use-her', default=True, type=str2bool)
-
-if __name__ == "__main__":
-    run_policy(HerRunSettings())
diff --git a/rl-toolkit/examples/dev/her/fetch_reach.cmd b/rl-toolkit/examples/dev/her/fetch_reach.cmd
deleted file mode 100644
index 0ac4b7f..0000000
--- a/rl-toolkit/examples/dev/her/fetch_reach.cmd
+++ /dev/null
@@ -1,4 +0,0 @@
-# Pretending like num-cpu=16 and num-envs=2 and batch-size 128
-# num-steps needs to be set to the episode length (50 for reacher)
-# each update needs 16 episodes (for 8 threads) so 16 * 8 * 50
-python tests/dev/her/def.py --prefix 'her-test' --num-env-steps 5e6 --env-name "FetchReach-v1" --log-smooth-len 10 --save-interval -1 --lr 0.001 --critic-lr 0.001 --tau 0.05 --warmup-steps 0 --update-every 1 --trans-buffer-size 1000000 --batch-size 128 --linear-lr-decay False --max-grad-norm -1 --noise-std 0.1 --noise-type gaussian --num-processes 32 --num-steps 200 --updates-per-batch 40 --log-interval 1 --n-rnd-steps 0 --rnd-prob 0.2 --num-render 0 --eval-interval 25 --gamma 0.98
diff --git a/rl-toolkit/examples/dev/mini/README.md b/rl-toolkit/examples/dev/mini/README.md
deleted file mode 100644
index 800e880..0000000
--- a/rl-toolkit/examples/dev/mini/README.md
+++ /dev/null
@@ -1,2 +0,0 @@
-For running simple enviornments such as:
-- `CartPole-v0`: Solved when policy obtains a reward of at least 195.0 over 100 consecutive trials.
diff --git a/rl-toolkit/examples/dev/mini/a2c.cmd b/rl-toolkit/examples/dev/mini/a2c.cmd
deleted file mode 100644
index 4bf5932..0000000
--- a/rl-toolkit/examples/dev/mini/a2c.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --eval-interval -1 --save-interval -1 --num-env-steps 50000 --prefix 'a2c' --alg 'a2c' --linear-lr-decay False --mlp-num-layers 2 --policy-hidden-size 16 --reward-smooth-len 5 --num-processes 1
diff --git a/rl-toolkit/examples/dev/mini/ppo.cmd b/rl-toolkit/examples/dev/mini/ppo.cmd
deleted file mode 100644
index fe11612..0000000
--- a/rl-toolkit/examples/dev/mini/ppo.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --eval-interval -1 --save-interval -1 --num-env-steps 50000 --prefix 'ppo' --alg 'ppo' --linear-lr-decay False --mlp-num-layers 2 --policy-hidden-size 16 --reward-smooth-len 5 --num-processes 1
diff --git a/rl-toolkit/examples/dev/mini/q_learn.cmd b/rl-toolkit/examples/dev/mini/q_learn.cmd
deleted file mode 100644
index b9daafa..0000000
--- a/rl-toolkit/examples/dev/mini/q_learn.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --eval-interval -1 --save-interval -1 --num-env-steps 50000 --prefix 'dqn' --alg 'q_learn' --linear-lr-decay False --mlp-num-layers 2 --policy-hidden-size 16 --reward-smooth-len 5
diff --git a/rl-toolkit/examples/dev/mini/sarsa.cmd b/rl-toolkit/examples/dev/mini/sarsa.cmd
deleted file mode 100644
index 9e18d37..0000000
--- a/rl-toolkit/examples/dev/mini/sarsa.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --eval-interval -1 --save-interval -1 --num-env-steps 50000 --prefix 'sarsa' --alg 'sarsa' --linear-lr-decay False --mlp-num-layers 2 --policy-hidden-size 16 --reward-smooth-len 5
diff --git a/rl-toolkit/examples/dev/mj/README.md b/rl-toolkit/examples/dev/mj/README.md
deleted file mode 100644
index a93cfdc..0000000
--- a/rl-toolkit/examples/dev/mj/README.md
+++ /dev/null
@@ -1,2 +0,0 @@
-Environments to try out here: 
-- `Hopper-v3` 
diff --git a/rl-toolkit/examples/dev/mj/bc.cmd b/rl-toolkit/examples/dev/mj/bc.cmd
deleted file mode 100644
index a58075a..0000000
--- a/rl-toolkit/examples/dev/mj/bc.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --prefix bc --use-proper-time-limits --alg bc 
diff --git a/rl-toolkit/examples/dev/mj/ddpg.cmd b/rl-toolkit/examples/dev/mj/ddpg.cmd
deleted file mode 100644
index f6b9330..0000000
--- a/rl-toolkit/examples/dev/mj/ddpg.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --prefix ddpg  --use-proper-time-limits --eval-interval -1 --save-interval -1 --alg ddpg --env-log-dir ~/tmp --trans-buffer-size 1000000 --batch-size 128 --lr 1e-4 --critic-lr 1e-3 --num-env-steps 3000000 --noise-std 0.1 --warmup-steps 0
diff --git a/rl-toolkit/examples/dev/mj/ddpg_mc_cont.cmd b/rl-toolkit/examples/dev/mj/ddpg_mc_cont.cmd
deleted file mode 100644
index a560632..0000000
--- a/rl-toolkit/examples/dev/mj/ddpg_mc_cont.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --prefix ddpg  --eval-interval -1 --save-interval -1  --num-env-steps 3000000 --alg ddpg --env-name MountainCarContinuous-v0 --env-log-dir ~/tmp --trans-buffer-size 10000 --batch-size 64 --lr 1e-4 --critic-lr 1e-3 --num-env-steps 1e5
diff --git a/rl-toolkit/examples/dev/mj/gail.cmd b/rl-toolkit/examples/dev/mj/gail.cmd
deleted file mode 100644
index 1bed88e..0000000
--- a/rl-toolkit/examples/dev/mj/gail.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run.py --prefix gail --use-proper-time-limits --alg gail 
diff --git a/rl-toolkit/examples/dev/mj/ppo.cmd b/rl-toolkit/examples/dev/mj/ppo.cmd
deleted file mode 100644
index 540df76..0000000
--- a/rl-toolkit/examples/dev/mj/ppo.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/run_alg.py --prefix ppo --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --alg ppo
diff --git a/rl-toolkit/examples/dev/option_critic/cartpole.cmd b/rl-toolkit/examples/dev/option_critic/cartpole.cmd
deleted file mode 100644
index e13f2b2..0000000
--- a/rl-toolkit/examples/dev/option_critic/cartpole.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/option_critic/def.py --prefix 'option-critic-test' --env-name CartPole-v0 --eval-interval -1 --log-smooth-len 10 --save-interval -1 --linear-lr-decay False --max-grad-norm -1 --batch-size 64 --lr 0.005 --normalize-env False 
diff --git a/rl-toolkit/examples/dev/option_critic/def.py b/rl-toolkit/examples/dev/option_critic/def.py
deleted file mode 100644
index 9cb553e..0000000
--- a/rl-toolkit/examples/dev/option_critic/def.py
+++ /dev/null
@@ -1,17 +0,0 @@
-import sys
-sys.path.insert(0, './')
-
-from rlf.algos.hier.option_critic import OptionCritic
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.options_policy import OptionsPolicy
-
-class OptionCriticRunSettings(TestRunSettings):
-    def get_policy(self):
-        return OptionsPolicy()
-
-    def get_algo(self):
-        return OptionCritic()
-
-if __name__ == "__main__":
-    run_policy(OptionCriticRunSettings())
diff --git a/rl-toolkit/examples/dev/option_critic/gw.cmd b/rl-toolkit/examples/dev/option_critic/gw.cmd
deleted file mode 100644
index a7dbd79..0000000
--- a/rl-toolkit/examples/dev/option_critic/gw.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/option_critic/def.py --prefix 'option-critic-test' --env-name MiniGrid-Empty-5x5-v0 --eval-interval -1 --log-smooth-len 10 --save-interval -1 --linear-lr-decay False --max-grad-norm -1 --batch-size 64 --lr 0.0005 --normalize-env False
diff --git a/rl-toolkit/examples/dev/poker/main.py b/rl-toolkit/examples/dev/poker/main.py
deleted file mode 100644
index 029ec1f..0000000
--- a/rl-toolkit/examples/dev/poker/main.py
+++ /dev/null
@@ -1,21 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import QLearning
-from rlf import DQN
-from tests.test_run_settings import TestRunSettings
-from rlf import run_policy
-import rlf.envs.neuron_poker
-
-
-class DqnRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DQN()
-
-    def get_algo(self):
-        return QLearning()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-
-if __name__ == "__main__":
-    run_policy(DqnRunSettings())
diff --git a/rl-toolkit/examples/dev/poker/run.cmd b/rl-toolkit/examples/dev/poker/run.cmd
deleted file mode 100644
index 4ae4378..0000000
--- a/rl-toolkit/examples/dev/poker/run.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/poker/main.py --prefix 'poker' --env-name Poker-v0 --save-interval -1 --normalize-env False --eval-interval -1 --num-env-steps 1e5
diff --git a/rl-toolkit/examples/dev/policy_iter/def.py b/rl-toolkit/examples/dev/policy_iter/def.py
deleted file mode 100644
index ae920a8..0000000
--- a/rl-toolkit/examples/dev/policy_iter/def.py
+++ /dev/null
@@ -1,30 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from rlf.policies.tabular.tabular_policy import TabularPolicy
-from rlf.algos.tabular.policy_iteration import PolicyIteration
-from rlf.algos.tabular.value_iteration import ValueIteration
-from tests.test_run_settings import TestRunSettings
-from rlf.args import str2bool
-from rlf.rl.loggers.plt_logger import PltLogger
-
-class PolicyIterRunSettings(TestRunSettings):
-    def get_policy(self):
-        return TabularPolicy()
-
-    def get_algo(self):
-        if self.base_args.value_iter:
-            return ValueIteration()
-        else:
-            return PolicyIteration()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--value-iter', default=False, type=str2bool)
-
-    def get_logger(self):
-        return PltLogger(['eval_train_r'], '# Updates', ['Reward'], ['Frozen Lake'])
-
-
-
-run_policy(PolicyIterRunSettings())
diff --git a/rl-toolkit/examples/dev/policy_iter/frozen_lake.cmd b/rl-toolkit/examples/dev/policy_iter/frozen_lake.cmd
deleted file mode 100644
index 934b3fd..0000000
--- a/rl-toolkit/examples/dev/policy_iter/frozen_lake.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/policy_iter/def.py --prefix 'policy-iter-test' --env-name FrozenLake8x8-v0 --log-smooth-len 10 --save-interval -1 --normalize-env False --num-render 0 --eval-interval 1 --num-render 0 --num-steps 1 --log-interval 1 --num-iters 100
diff --git a/rl-toolkit/examples/dev/ppo/gw.cmd b/rl-toolkit/examples/dev/ppo/gw.cmd
deleted file mode 100644
index 7f7dec3..0000000
--- a/rl-toolkit/examples/dev/ppo/gw.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/ppo/main.py --prefix 'ppo-gw' --env-name MiniGrid-FourRooms-v0 --save-interval -1 --normalize-env False --eval-interval 100  --eval-num-processes 1
diff --git a/rl-toolkit/examples/dev/ppo/main.py b/rl-toolkit/examples/dev/ppo/main.py
deleted file mode 100644
index cece678..0000000
--- a/rl-toolkit/examples/dev/ppo/main.py
+++ /dev/null
@@ -1,203 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import PPO
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-from rlf.envs.env_interface import EnvInterfaceWrapper, register_env_interface
-from rlf.envs.minigrid_interface import MinigridInterface
-from rlf.algos.nested_algo import NestedAlgo
-from abc import ABC, abstractmethod
-import gym
-from enum import IntEnum
-import rlf.rl.utils as rutils
-from rlf import BaseNetPolicy
-from rlf.rl.multi_runner import MultiRunner
-
-
-class ForwardStorage:
-    Data = ['TEST!']
-    @staticmethod
-    def sample():
-        return ForwardStorage.Data[0]
-
-class PlayMode(IntEnum):
-    REAL = 0
-    BACK = 1
-
-
-class DoublePlaybackWrapper(gym.Wrapper):
-    def __init__(self, env):
-        super().__init__(env)
-        self._starting = None
-        self._play_mode = PlayMode.REAL
-        #self.observation_space = rutils.combine_spaces(
-        #        self.observation_space,
-        #        "mode", gym.spaces.Discrete(2))
-
-    def step(self, a):
-        obs, reward, done, info = super().step(a)
-        #info['mode'] = self._play_mode
-        print('Sampling!', ForwardStorage.sample())
-        if done:
-            if self._play_mode == PlayMode.REAL:
-                self._play_mode = PlayMode.BACK
-            elif self._play_mode == PlayMode.BACK:
-                self._play_mode = PlayMode.REAL
-        return self._mod_obs(obs), reward, done, info
-
-    def _mod_obs(self, obs):
-        #obs = rutils.combine_obs(obs, "mode", self._play_mode)
-        return obs
-
-    @abstractmethod
-    def _set_state(self, obs):
-        pass
-
-    def _get_cur_state(self, obs):
-        return obs
-
-    def reset(self):
-        obs = super().reset()
-        #if self._play_mode == PlayMode.BACK:
-        #    self._set_state(self._starting)
-        self._starting = self._get_cur_state(obs)
-        return self._mod_obs(obs)
-
-class MinigridPlaybackWrapper(DoublePlaybackWrapper):
-    def _set_state(self, obs):
-        # Extract the agent and goal positions
-        grid, start, direction = obs
-        self.env.env.grid = grid
-        self.env.env.agent_pos = start
-        self.env.env.agent_dir = direction
-
-    def _get_cur_state(self, obs):
-        return (self.env.grid, self.env.agent_pos, self.env.agent_dir)
-
-
-class DoublePlaybackEnvInterface(EnvInterfaceWrapper):
-    def __init__(self, args):
-        env = super().__init__(args, MinigridInterface)
-
-    def create_from_id(self, env_id):
-        env = super().create_from_id(env_id)
-        env = MinigridPlaybackWrapper(env)
-        return env
-
-register_env_interface("^MiniGrid", DoublePlaybackEnvInterface)
-
-#class DoublePlayUpdater(NestedAlgo):
-#    def __init__(self, updater1, updater2):
-#        super().__init__([updater1, updater2], 0)
-#
-#    def get_storage_buffer(self, policy, envs, args):
-#        return SwitchingNestedStorage(
-#                self.modules[0].get_storage_buffer(policy, envs, args),
-#                self.modules[1].get_storage_buffer(policy, envs, args))
-#
-#
-#class SwitchingNestedStorage(NestedStorage):
-#    def __init__(self, main_storage, back_storage):
-#        super().__init__({
-#            'main': main_storage,
-#            'back': back_storage
-#            }, 'main')
-#
-#
-#class DoublePolicy(BasePolicy):
-#    def __init__(self, policy1, policy2):
-#        super().__init__()
-#        self.policy1 = policy1
-#        self.policy2 = policy2
-#        self.policy1.set_arg_prefix('A')
-#        self.policy2.set_arg_prefix('B')
-#
-#    def init(self, obs_space, action_space, args):
-#        super().init(obs_space, action_space, args)
-#        self.policy1.init(obs_space, action_space, args)
-#        self.policy2.init(obs_space, action_space, args)
-#
-#    def get_storage_hidden_states(self):
-#        return self.policy1.get_storage_hidden_states()
-#
-#
-#    def get_action(self, state, add_state, hxs, masks, step_info):
-#        pass
-#
-#    def get_add_args(self, parser):
-#        self.policy1.get_add_args(parser)
-#        self.policy2.get_add_args(parser)
-#
-#    def load_from_checkpoint(self, checkpointer):
-#        self.policy1.load_from_checkpoint(checkpointer)
-#        self.policy2.load_from_checkpoint(checkpointer)
-#
-#    def save_to_checkpoint(self, checkpointer):
-#        self.policy1.save_to_checkpoint(checkpointer)
-#        self.policy2.save_to_checkpoint(checkpointer)
-#
-#    def load_resume(self, checkpointer):
-#        self.policy1.load_resume(checkpointer)
-#        self.policy2.load_resume(checkpointer)
-#
-#    def set_env_ref(self, envs):
-#        self.policy1.set_env_ref(envs)
-#        self.policy2.set_env_ref(envs)
-#
-#    def to(self, device):
-#        self.policy1 = self.policy1.to(envs)
-#        self.policy2 = self.policy2.to(envs)
-#        return self
-#
-#    def eval(self):
-#        self.policy1.eval()
-#        self.policy2.eval()
-#
-#    def train(self):
-#        self.policy1.train()
-#        self.policy2.train()
-
-
-class SimpleRunSettings(TestRunSettings):
-    def __init__(self, is_forward, use_log=None, use_args=None):
-        super().__init__()
-        self.is_forward = is_forward
-        self.use_log = use_log
-        self.use_args = use_args
-
-    def _get_env_interface(self, args, task_id=None):
-        if not self.is_forward:
-            ei = DoublePlaybackEnvInterface(args)
-            ei.setup(args, task_id)
-            return ei
-        return super()._get_env_interface(args)
-
-    def get_policy(self):
-        return DistActorCritic()
-
-    def _sys_setup(self, add_args, ray_create):
-        if not self.is_forward:
-            return self.use_args, self.use_log
-        return _sys_setup(add_args, ray_create)
-
-    def get_algo(self):
-        return PPO()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-
-frunsts = SimpleRunSettings(True)
-frunner = frunsts.create_runner()
-
-brunsts = SimpleRunSettings(False, frunner.log, frunner.args)
-brunner = brunsts.create_runner()
-
-class DualRunner(MultiRunner):
-    def training_iter(self, update_iter):
-        self.forward_runner.training_iter(update_iter)
-
-if __name__ == "__main__":
-    run_policy(PPORunSettings())
-
-
diff --git a/rl-toolkit/examples/dev/ppo_atari.cmd b/rl-toolkit/examples/dev/ppo_atari.cmd
deleted file mode 100644
index 8f33109..0000000
--- a/rl-toolkit/examples/dev/ppo_atari.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python examples/run_ppo.py --lr 2.5e-4 --num-mini-batch 4 --log-interval 1 --linear-lr-decay True --entropy-coef 0.01 --eval-interval -1 --save-interval -1 --prefix 'ppo'  --num-processes 8 --num-env-steps 1e7
diff --git a/rl-toolkit/examples/dev/run_alg.py b/rl-toolkit/examples/dev/run_alg.py
deleted file mode 100644
index 053775e..0000000
--- a/rl-toolkit/examples/dev/run_alg.py
+++ /dev/null
@@ -1,107 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-from rlf.algos.il.bc import BehavioralCloning
-from rlf.algos.il.gail import GAIL
-from rlf.algos.il.bco import BehavioralCloningFromObs
-from rlf.algos.off_policy.ddpg import DDPG
-from rlf.algos.on_policy.sarsa import SARSA
-from rlf.algos.on_policy.reinforce import REINFORCE
-from rlf.algos.on_policy.a2c import A2C
-from rlf.algos.off_policy.q_learning import QLearning
-from rlf.algos.on_policy.ppo import PPO
-from rlf import BaseAlgo
-from rlf.algos.il.gaifo import GAIFO
-from rlf.policies.basic_policy import BasicPolicy
-from rlf.policies.random_policy import RandomPolicy
-from rlf.policies.dqn import DQN
-from rlf.policies.actor_critic.reg_actor_critic import RegActorCritic
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-from rlf.run_settings import RunSettings
-from rlf import run_policy
-from rlf.rl.loggers.wb_logger import WbLogger
-from rlf.rl.loggers.base_logger import BaseLogger
-
-DIST_ACTOR_CRITIC = ('ppo', 'a2c', 'reinforce', 'gail', 'gaifo')
-REG_ACTOR_CRITIC = ('ddpg',)
-NO_CRITIC = ('q_learn', 'sarsa')
-BASIC_POLICY = ('bc','bco')
-RND_POLICY = ('rnd',)
-
-
-class DefaultRunSettings(RunSettings):
-    def get_policy(self):
-        alg = self.base_args.alg
-        if alg in DIST_ACTOR_CRITIC:
-            return DistActorCritic()
-        elif alg in REG_ACTOR_CRITIC:
-            # DDPG is hard to train, make some changes to the base actor
-            if alg == 'ddpg' and self.base_args.env_name == 'Hopper-v3':
-                return RegActorCritic(
-                    get_actor_head_fn=lambda _, i_shape: MLPBase(
-                        i_shape[0], False, (128, 128)),
-                    get_critic_head_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                        i_shape[0], (128, 128), a_space.shape[0])
-                )
-
-            if alg == 'ddpg' and self.base_args.env_name == 'MountainCarContinuous-v0':
-                return RegActorCritic(
-                    get_actor_head_fn=lambda _, i_shape: MLPBase(
-                        i_shape[0], False, (400, 300)),
-                    get_critic_head_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                        i_shape[0], (400, 300), a_space.shape[0])
-                )
-            else:
-                return RegActorCritic()
-        elif alg in NO_CRITIC:
-            return DQN()
-        elif alg in BASIC_POLICY:
-            return BasicPolicy()
-        elif alg in RND_POLICY:
-            return RandomPolicy()
-        else:
-            raise ValueError('Unrecognized alg for policy architecture')
-
-    def get_algo(self):
-        alg = self.base_args.alg
-        if alg == 'ppo':
-            return PPO()
-        elif alg == 'a2c':
-            return A2C()
-        elif alg == 'reinforce':
-            return REINFORCE()
-        elif alg == 'q_learn':
-            return QLearning()
-        elif alg == 'sarsa':
-            return SARSA()
-        elif alg == 'ddpg':
-            return DDPG()
-        elif alg == 'bc':
-            return BehavioralCloning()
-        elif alg == 'gail':
-            return GAIL()
-        elif alg == 'gaifo':
-            return GAIFO()
-        elif alg == 'rnd':
-            return BaseAlgo()
-        elif alg == 'bco':
-            return BehavioralCloningFromObs()
-        else:
-            raise ValueError('Unrecognized alg for optimizer')
-
-    def get_logger(self):
-        if self.base_args.no_wb:
-            return BaseLogger()
-        else:
-            return WbLogger()
-
-    def get_config_file(self):
-        return './tests/config.yaml'
-
-    def get_add_args(self, parser):
-        parser.add_argument('--alg')
-        parser.add_argument('--no-wb', default=False, action='store_true')
-        parser.add_argument('--env-name')
-
-
-run_policy(DefaultRunSettings())
diff --git a/rl-toolkit/examples/dev/sac/hopper.cmd b/rl-toolkit/examples/dev/sac/hopper.cmd
deleted file mode 100644
index 3817103..0000000
--- a/rl-toolkit/examples/dev/sac/hopper.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/sac/main.py --prefix 'sac' --env-name Hopper-v3 --save-interval -1 --init-temperature 0.1 --alpha-lr 1e-4 --lr 1e-4 --critic-lr 1e-4 --actor-update-freq 1 --tau 0.005 --critic-update-freq 2 --batch-size 256 --num-env-steps 1e6 --trans-buffer-size 1e6 --max-grad-norm -1 --linear-lr-decay False --hidden-dim 256 --n-rnd-steps 1000 --eval-interval 1000 --num-render 0 --eval-num-processes 40 --num-eval 10 --num-env-steps 3e6 --eps 1e-8 --normalize-env False
diff --git a/rl-toolkit/examples/dev/sac/main.py b/rl-toolkit/examples/dev/sac/main.py
deleted file mode 100644
index b645bde..0000000
--- a/rl-toolkit/examples/dev/sac/main.py
+++ /dev/null
@@ -1,35 +0,0 @@
-"""
-Code is heavily based off of https://github.com/denisyarats/pytorch_sac.
-The license is at `rlf/algos/off_policy/denis_yarats_LICENSE.md`
-"""
-import sys
-sys.path.insert(0, './')
-from rlf.algos.off_policy.sac import SAC
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-import torch.nn as nn
-import torch.nn.functional as F
-from rlf.rl.model import BaseNet, IdentityBase, MLPBase
-from rlf.policies.actor_critic.dist_actor_q import DistActorQ, get_sac_actor, get_sac_critic
-import torch
-import math
-from functools import partial
-
-
-class SACRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DistActorQ(
-				get_critic_fn=partial(get_sac_critic, hidden_dim=self.base_args.hidden_dim),
-                get_actor_fn=partial(get_sac_actor, hidden_dim=self.base_args.hidden_dim)
-                )
-
-    def get_algo(self):
-        return SAC()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--hidden-dim', type=int, default=1024)
-
-if __name__ == "__main__":
-    run_policy(SACRunSettings())
diff --git a/rl-toolkit/examples/dev/sqil/halfcheetah.cmd b/rl-toolkit/examples/dev/sqil/halfcheetah.cmd
deleted file mode 100644
index 59c160b..0000000
--- a/rl-toolkit/examples/dev/sqil/halfcheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/sqil/main.py --use-proper-time-limits --prefix 'sqil' --env-name HalfCheetah-v2 --save-interval -1 --normalize-env False --eval-interval 10000 --num-env-steps 1e6 --trans-buffer-size 1e6 --traj-batch-size 128 --n-rnd-steps 1000 --traj-load-path tests/expert_demonstrations/halfcheetah_50ep.pt --linear-lr-decay False --max-grad-norm -1 --eps 1e-8 --eval-num-processes 40 --num-eval 10 --num-render 0
diff --git a/rl-toolkit/examples/dev/sqil/halfcheetah_old.cmd b/rl-toolkit/examples/dev/sqil/halfcheetah_old.cmd
deleted file mode 100644
index 7071309..0000000
--- a/rl-toolkit/examples/dev/sqil/halfcheetah_old.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/sqil/main.py --use-proper-time-limits --prefix 'sqil' --env-name HalfCheetah-v1 --save-interval -1 --normalize-env False --eval-interval 10000 --num-env-steps 1e6 --trans-buffer-size 1e6 --traj-batch-size 128 --n-rnd-steps 1000 --traj-load-path /home/aszot/expert_datasets/stochastic.trpo.HalfCheetah.0.00.npz --linear-lr-decay False --max-grad-norm -1 --eps 1e-8 --eval-num-processes 40 --num-eval 10 --num-render 0
diff --git a/rl-toolkit/examples/dev/sqil/main.py b/rl-toolkit/examples/dev/sqil/main.py
deleted file mode 100644
index 51d3528..0000000
--- a/rl-toolkit/examples/dev/sqil/main.py
+++ /dev/null
@@ -1,19 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_q import DistActorQ, get_sac_actor, get_sac_critic
-from rlf.algos.il.sqil import SQIL
-from functools import partial
-
-class SqilRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DistActorQ(
-				get_critic_fn=partial(get_sac_critic, hidden_dim=256),
-                get_actor_fn=partial(get_sac_actor, hidden_dim=256))
-
-    def get_algo(self):
-        return SQIL()
-
-if __name__ == "__main__":
-    run_policy(SqilRunSettings())
diff --git a/rl-toolkit/examples/dev/sql/cartpole.cmd b/rl-toolkit/examples/dev/sql/cartpole.cmd
deleted file mode 100644
index a6775c9..0000000
--- a/rl-toolkit/examples/dev/sql/cartpole.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/dev/sql/def.py --prefix 'sql' --env-name CartPole-v0 --save-interval -1 --normalize-env False --eval-interval -1 --num-env-steps 1e5
diff --git a/rl-toolkit/examples/dev/sql/def.py b/rl-toolkit/examples/dev/sql/def.py
deleted file mode 100644
index 948fcc9..0000000
--- a/rl-toolkit/examples/dev/sql/def.py
+++ /dev/null
@@ -1,17 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from rlf.algos.off_policy.soft_qlearning import SoftQLearning
-from rlf.policies.svgd_policy import SVGDPolicy
-from tests.test_run_settings import TestRunSettings
-from rlf.args import str2bool
-from rlf.rl.loggers.plt_logger import PltLogger
-
-class SoftQLearningRunSettings(TestRunSettings):
-    def get_policy(self):
-        return SVGDPolicy()
-
-    def get_algo(self):
-        return SoftQLearning()
-
-run_policy(SoftQLearningRunSettings())
diff --git a/rl-toolkit/examples/expert_demonstrations/halfcheetah_50ep.pt b/rl-toolkit/examples/expert_demonstrations/halfcheetah_50ep.pt
deleted file mode 100644
index 9964d18..0000000
Binary files a/rl-toolkit/examples/expert_demonstrations/halfcheetah_50ep.pt and /dev/null differ
diff --git a/rl-toolkit/examples/gdev/any.py b/rl-toolkit/examples/gdev/any.py
deleted file mode 100644
index 7e9c6d0..0000000
--- a/rl-toolkit/examples/gdev/any.py
+++ /dev/null
@@ -1,207 +0,0 @@
-import sys
-sys.path.insert(0, './')
-import os
-import os.path as osp
-
-import torch
-import numpy as np
-import argparse
-import string
-import random
-import datetime
-
-from garage import wrap_experiment
-from garage.experiment.deterministic import set_seed
-from garage.torch.algos import PPO
-from garage.torch.policies import GaussianMLPPolicy
-from garage.torch.value_functions import GaussianMLPValueFunction
-from garage.trainer import Trainer
-import pybullet_envs  # noqa: F401  # pylint: disable=unused-import
-import pybulletgym
-from garage.torch import set_gpu_mode
-from garage.sampler import LocalSampler
-from garage.sampler import VecWorker
-from garage.sampler import DefaultWorker
-from garage.sampler import MultiprocessingSampler
-from garage.envs import GymEnv, normalize
-
-from garage.torch.algos import SAC
-from garage.torch.policies import TanhGaussianMLPPolicy
-from garage.torch.q_functions import ContinuousMLPQFunction
-from garage.replay_buffer import PathBuffer
-from garage.torch.optimizers import OptimizerWrapper
-from torch import nn
-from torch.nn import functional as F
-from rlf.garage.auto_arg import convert_to_args, convert_kwargs
-from rlf.args import str2bool
-from rlf.exp_mgr import config_mgr
-
-from dowel import logger
-from rlf.garage.wb_logger import WbOutput
-from rlf.garage.std_logger import StdLogger
-
-def setup_def_parser():
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--seed', type=int, default=1)
-    parser.add_argument('--n-epochs', type=int, default=1000)
-    parser.add_argument('--batch-size', type=int, default=1000)
-    parser.add_argument('--hidden-dim', type=int, default=256)
-    parser.add_argument('--log-interval', type=int, default=10)
-    parser.add_argument('--depth', type=int, default=2)
-    parser.add_argument('--env-name', type=str, required=True)
-    parser.add_argument('--prefix', type=str, default='debug')
-    parser.add_argument('--env-norm', type=str2bool, default=False)
-    parser.add_argument('--cuda', type=str2bool, default=False)
-    parser.add_argument('--no-wb', action='store_true', default=False)
-    parser.add_argument('--alg', type=str, required=True)
-    return parser
-
-
-def ppo_args(parser):
-    convert_to_args(PPO, parser)
-    parser.add_argument('--policy-lr', type=float, default=3e-4)
-    parser.add_argument('--vf-lr', type=float, default=3e-4)
-    parser.add_argument('--n-minibatches', type=float, default=10)
-    parser.add_argument('--minibatch-size', type=float, default=None)
-
-
-def ppo_setup(env, trainer, args):
-    policy = GaussianMLPPolicy(env.spec,
-                               hidden_sizes=[args.hidden_dim]*args.depth,
-                               hidden_nonlinearity=torch.tanh,
-                               output_nonlinearity=None)
-
-    value_function = GaussianMLPValueFunction(env_spec=env.spec,
-                                              hidden_sizes=[args.hidden_dim]*args.depth,
-                                              hidden_nonlinearity=torch.tanh,
-                                              output_nonlinearity=None)
-
-    algo = PPO(env_spec=env.spec,
-            policy=policy,
-            value_function=value_function,
-            policy_optimizer=OptimizerWrapper(
-                (torch.optim.Adam, dict(lr=args.policy_lr)),
-                policy,
-                max_optimization_epochs=args.n_minibatches,
-                minibatch_size=args.minibatch_size),
-            vf_optimizer=OptimizerWrapper(
-                (torch.optim.Adam, dict(lr=args.vf_lr)),
-                value_function,
-                max_optimization_epochs=args.n_minibatches,
-                minibatch_size=args.minibatch_size),
-            **convert_kwargs(args, PPO))
-    trainer.setup(algo, env, sampler_cls=LocalSampler, worker_class=VecWorker,
-            worker_args={'n_envs': 8})
-    return algo
-
-
-def sac_args(parser):
-    convert_to_args(SAC, parser)
-    parser.add_argument('--buffer-size', type=float, default=1e6)
-    parser.add_argument('--gradient-steps-per-itr', type=int, default=1000)
-
-
-def sac_setup(env, trainer, args):
-    policy = TanhGaussianMLPPolicy(
-        env_spec=env.spec,
-        hidden_sizes=[args.hidden_dim]*args.depth,
-        hidden_nonlinearity=nn.ReLU,
-        output_nonlinearity=None,
-        min_std=np.exp(-20.),
-        max_std=np.exp(2.),
-    )
-
-    qf1 = ContinuousMLPQFunction(env_spec=env.spec,
-                                 hidden_sizes=[args.hidden_dim]*args.depth,
-                                 hidden_nonlinearity=F.relu)
-
-    qf2 = ContinuousMLPQFunction(env_spec=env.spec,
-                                 hidden_sizes=[args.hidden_dim]*args.depth,
-                                 hidden_nonlinearity=F.relu)
-
-    replay_buffer = PathBuffer(capacity_in_transitions=int(args.buffer_size))
-
-    sac = SAC(env_spec=env.spec,
-              policy=policy,
-              qf1=qf1,
-              qf2=qf2,
-              **convert_kwargs(args, SAC))
-
-    trainer.setup(algo=sac, env=env, sampler_cls=LocalSampler)
-    return sac
-
-
-USE_FNS = {
-        'ppo': (ppo_args, ppo_setup),
-        'sac': (sac_args, sac_setup),
-        }
-
-def get_env_id(args):
-    upper_case = [c for c in args.env_name if c.isupper()]
-    if len(upper_case) == 0:
-        return ''.join([word[0] for word in args.env_name.split(".")])
-    else:
-        return ''.join(upper_case)
-
-def create_prefix(args):
-    assert args.prefix is not None and args.prefix != '', 'Must specify a prefix'
-    d = datetime.datetime.today()
-    date_id = '%i%i' % (d.month, d.day)
-    env_id = get_env_id(args)
-    rnd_id = ''.join(random.sample(
-        string.ascii_uppercase + string.digits, k=2))
-    before = ('%s-%s-%s-%s-' %
-              (date_id, env_id, args.seed, rnd_id))
-
-    if args.prefix != 'debug' and args.prefix != 'NONE':
-        prefix = before + args.prefix
-        print('Assigning full prefix %s' % prefix)
-    else:
-        prefix = args.prefix
-    return prefix
-
-def setup_launcher():
-    config_dir = osp.dirname(osp.realpath(__file__))
-    config_path = osp.join(config_dir, 'config.yaml')
-    config_mgr.init(config_path)
-    parser = setup_def_parser()
-
-    # First parse the regular args
-    base_args, _ = parser.parse_known_args()
-    get_args, get_algo = USE_FNS[base_args.alg]
-    use_prefix = create_prefix(base_args)
-
-    @wrap_experiment(archive_launch_repo=False, snapshot_mode='none', name=use_prefix)
-    def alg_train(ctxt=None):
-        get_args(parser)
-        args = parser.parse_args()
-        args.prefix = use_prefix
-
-        set_seed(args.seed)
-        env = GymEnv(args.env_name)
-        if args.env_norm:
-            env = normalize(env)
-
-        trainer = Trainer(ctxt)
-
-        logger.remove_all()
-        logger.add_output(StdLogger(args.log_interval))
-
-        if not args.no_wb:
-            wb_logger = WbOutput(args.log_interval, base_args)
-            logger.add_output(wb_logger)
-
-        algo = get_algo(env, trainer, args)
-
-        if args.cuda:
-            set_gpu_mode(True)
-            algo.to()
-        else:
-            set_gpu_mode(False)
-
-        trainer.train(n_epochs=args.n_epochs, batch_size=args.batch_size)
-
-    return alg_train
-
-launcher = setup_launcher()
-launcher()
diff --git a/rl-toolkit/examples/gdev/config.yaml b/rl-toolkit/examples/gdev/config.yaml
deleted file mode 100644
index 82484a4..0000000
--- a/rl-toolkit/examples/gdev/config.yaml
+++ /dev/null
@@ -1,5 +0,0 @@
-conda_env: "rlf"
-cmds_loc: "tests/gdev/cmds/"
-proj_name: "andrew-random"
-wb_entity: "aszot"
-base_data_dir: "/srv/share/aszot3/rl-toolkit"
diff --git a/rl-toolkit/examples/test_cmds/__init__.py b/rl-toolkit/examples/test_cmds/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/rl-toolkit/examples/test_cmds/bco/README.md b/rl-toolkit/examples/test_cmds/bco/README.md
deleted file mode 100644
index 23034aa..0000000
--- a/rl-toolkit/examples/test_cmds/bco/README.md
+++ /dev/null
@@ -1,14 +0,0 @@
-Instructions on how the full process of first training a policy to get
-demonstrations and then performing IL.
-
-## HalfCheetah Instructions
-Steps: 
-1. Train expert `python -m rlf --cmd ppo/halfcheetah --cd 0 --cfg ./tests/config.yaml --cuda False --save-interval 10000000 --sess-id 0 --seed "31"`
-
-## CartPole Instructions
-Steps: 
-1. Train expert `py -m rlf --cfg tests/config.yaml --cuda False --cmd ppo/cartpole  --seed "31"  --sess-id 0 --save-interval 10000000`
-2. Evaluate expert and save expert dataset `py -m rlf --cfg tests/config.yaml
-   --cuda False --cmd ppo/cartpole  --seed "31"  --eval-only --load-file
-   ./data/trained_models/CartPole-v1/519-CP-31-T8-ppo-cartpole-test/model_389.pt --eval-save` 
-3. Run BCO with the path to your expert dataset `py -m rlf --cfg tests/config.yaml --cmd bco/cartpole --traj-load-path ./data/traj/CartPole-v1/520-CP-31-AE-ppo-cartpole-test/trajs.pt`
diff --git a/rl-toolkit/examples/test_cmds/bco/def.py b/rl-toolkit/examples/test_cmds/bco/def.py
deleted file mode 100644
index a803a1a..0000000
--- a/rl-toolkit/examples/test_cmds/bco/def.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import BehavioralCloningFromObs
-from rlf import BasicPolicy
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-from rlf.rl.model import MLPBase
-
-class BcoRunSettings(TestRunSettings):
-    def get_policy(self):
-        return BasicPolicy(
-                is_stoch=self.base_args.stoch_policy,
-                get_base_net_fn=lambda i_shape, recurrent: MLPBase(
-                    i_shape[0], False, (400, 300))
-                )
-
-    def get_algo(self):
-        return BehavioralCloningFromObs()
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--stoch-policy', default=False, action='store_true')
-
-if __name__ == "__main__":
-    run_policy(BcoRunSettings())
diff --git a/rl-toolkit/examples/test_cmds/bco/halfcheetah.cmd b/rl-toolkit/examples/test_cmds/bco/halfcheetah.cmd
deleted file mode 100644
index ca9fca8..0000000
--- a/rl-toolkit/examples/test_cmds/bco/halfcheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/bco/def.py --prefix bco --env-name HalfCheetah-v2 --eval-num-processes 32 --num-render 0 --num-env-steps 1e7 --bco-inv-lr 3e-4 --bco-alpha 50 --bco-alpha-size 200000 --bco-expl-steps 200000 --bco-inv-batch-size 128 --linear-lr-decay False --bc-state-norm --max-grad-norm -1 --normalize-env False --bc-num-epochs 10 --lr 3e-4 --bco-inv-epochs 1  --traj-load-path tests/expert_demonstrations/halfcheetah_50ep.pt --eval-interval 1 --log-smooth-len 10
diff --git a/rl-toolkit/examples/test_cmds/ddpg/def.py b/rl-toolkit/examples/test_cmds/ddpg/def.py
deleted file mode 100644
index 192ee14..0000000
--- a/rl-toolkit/examples/test_cmds/ddpg/def.py
+++ /dev/null
@@ -1,42 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import DDPG
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf import RegActorCritic
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-import torch.nn as nn
-import torch.nn.functional as F
-
-def reg_init(m):
-    return m
-
-def get_actor_head(hidden_dim, action_dim):
-    return nn.Sequential(
-            nn.Linear(hidden_dim, action_dim),
-            nn.Tanh())
-
-class DDPGRunSettings(TestRunSettings):
-    def get_policy(self):
-        if 'Pendulum' in self.base_args.env_name:
-            hidden_size = 128
-        else:
-            hidden_size = 256
-        return RegActorCritic(
-                get_actor_fn=lambda _, i_shape: MLPBase(
-                    i_shape[0], False, (hidden_size, hidden_size),
-                    weight_init=reg_init,
-                    get_activation=lambda: nn.ReLU()),
-                get_actor_head_fn=get_actor_head,
-                get_critic_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                    i_shape[0], (hidden_size, hidden_size), a_space.shape[0],
-                    weight_init=reg_init,
-                    get_activation=lambda: nn.ReLU()),
-                get_critic_head_fn = lambda hidden_dim: nn.Linear(hidden_dim, 1)
-                )
-
-    def get_algo(self):
-        return DDPG()
-
-if __name__ == "__main__":
-    run_policy(DDPGRunSettings())
diff --git a/rl-toolkit/examples/test_cmds/ddpg/pendulum.cmd b/rl-toolkit/examples/test_cmds/ddpg/pendulum.cmd
deleted file mode 100644
index e3fa43a..0000000
--- a/rl-toolkit/examples/test_cmds/ddpg/pendulum.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/ddpg/def.py --prefix 'ddpg-test' --num-env-steps 3e6 --env-name "Pendulum-v0" --eval-interval -1 --log-smooth-len 10 --save-interval -1 --lr 0.0005 --critic-lr 0.001 --tau 0.005 --warmup-steps 2000 --update-every 10 --trans-buffer-size 50000 --batch-size 32 --linear-lr-decay False --max-grad-norm -1 --noise-std 0.1 --noise-type uh
diff --git a/rl-toolkit/examples/test_cmds/gaifo/def.py b/rl-toolkit/examples/test_cmds/gaifo/def.py
deleted file mode 100644
index 9e13f2b..0000000
--- a/rl-toolkit/examples/test_cmds/gaifo/def.py
+++ /dev/null
@@ -1,29 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-from rlf.rl.model import MLPBase
-from rlf import GAIFO
-from rlf.rl.model import ConcatLayer
-import torch.nn as nn
-
-def get_discrim(in_shape, ac_dim):
-    return nn.Sequential(
-        ConcatLayer(-1),
-        nn.Linear(2*in_shape[0], 400), nn.Tanh(),
-        nn.Linear(400, 300), nn.Tanh(),
-        nn.Linear(300, 1))
-
-
-class GaifoRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DistActorCritic(
-                get_actor_fn=lambda _, i_shape: MLPBase(i_shape[0], False, (400, 300)),
-                get_critic_fn=lambda _, i_shape, a_shape: MLPBase(i_shape[0], False, (400, 300)))
-
-    def get_algo(self):
-        return GAIFO(get_discrim=get_discrim)
-
-if __name__ == "__main__":
-    run_policy(GaifoRunSettings())
diff --git a/rl-toolkit/examples/test_cmds/gaifo/halfcheetah.cmd b/rl-toolkit/examples/test_cmds/gaifo/halfcheetah.cmd
deleted file mode 100644
index af03b20..0000000
--- a/rl-toolkit/examples/test_cmds/gaifo/halfcheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/gaifo/def.py --prefix gaifo --env-name HalfCheetah-v2 --eval-num-processes 32 --num-render 0 --num-env-steps 1e7 --linear-lr-decay False --max-grad-norm -1 --lr 0.001 --disc-lr 0.001 --action-input False --entropy-coef 0.001 --eval-interval -1 --traj-load-path tests/expert_demonstrations/halfcheetah_50ep.pt --log-smooth-len 10 --linear-lr-decay True --n-gail-epochs 2
diff --git a/rl-toolkit/examples/test_cmds/gaifo_s/def.py b/rl-toolkit/examples/test_cmds/gaifo_s/def.py
deleted file mode 100644
index 6233f71..0000000
--- a/rl-toolkit/examples/test_cmds/gaifo_s/def.py
+++ /dev/null
@@ -1,25 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-from rlf.rl.model import MLPBase
-import torch.nn as nn
-from rlf import GAIL
-
-def get_discrim():
-    return nn.Sequential(
-            nn.Linear(400, 300), nn.Tanh(),
-            nn.Linear(300, 1)), 400
-
-class GaifoSRunSettings(TestRunSettings):
-    def get_policy(self):
-        return DistActorCritic(
-                get_actor_fn=lambda _, i_shape: MLPBase(i_shape[0], False, (400, 300)),
-                get_critic_fn=lambda _, i_shape, a_shape: MLPBase(i_shape[0], False, (400, 300)))
-
-    def get_algo(self):
-        return GAIL(get_discrim=get_discrim)
-
-if __name__ == "__main__":
-    run_policy(GaifoSRunSettings())
diff --git a/rl-toolkit/examples/test_cmds/gaifo_s/halfcheetah.cmd b/rl-toolkit/examples/test_cmds/gaifo_s/halfcheetah.cmd
deleted file mode 100644
index 879e4df..0000000
--- a/rl-toolkit/examples/test_cmds/gaifo_s/halfcheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/gaifo_s/def.py --prefix gaifo_s --env-name HalfCheetah-v2 --eval-num-processes 32 --num-render 0 --num-env-steps 1e7 --linear-lr-decay False --max-grad-norm -1 --lr 3e-4 --disc-lr 3e-4 --action-input False --entropy-coef 0.01 --eval-interval -1 --traj-load-path tests/expert_demonstrations/halfcheetah_50ep.pt --log-smooth-len 10 --reward-type 'gail' --gail-reward-norm True
diff --git a/rl-toolkit/examples/test_cmds/her/bit_flip.cmd b/rl-toolkit/examples/test_cmds/her/bit_flip.cmd
deleted file mode 100644
index 255d349..0000000
--- a/rl-toolkit/examples/test_cmds/her/bit_flip.cmd
+++ /dev/null
@@ -1,5 +0,0 @@
-# num-steps = bit-flip-n 
-# num-env-steps = 1000 * 16 * bit-flip-n
-# eps-decay = 45 * 16 * bit-flip-n
-# 20 bits
-python tests/test_cmds/her/def.py --prefix 'her-test' --env-name "BitFlip-v0" --log-smooth-len 10 --save-interval -1 --lr 0.001 --trans-buffer-size 1000000 --batch-size 32 --linear-lr-decay False --max-grad-norm -1 --num-processes 16 --updates-per-batch 40 --log-interval 5 --num-render 0 --eval-interval -1 --eps-start 0.2 --eps-end 0.02 --eps-decay 14400 --gamma 0.98 --normalize-env False --num-env-steps 320000 --num-steps 20 --bit-flip-n 20 
diff --git a/rl-toolkit/examples/test_cmds/her/def.py b/rl-toolkit/examples/test_cmds/her/def.py
deleted file mode 100644
index d0ef34c..0000000
--- a/rl-toolkit/examples/test_cmds/her/def.py
+++ /dev/null
@@ -1,64 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import DDPG
-from rlf import QLearning
-from rlf.algos.off_policy.her import create_her_storage_buff
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf import RegActorCritic
-from rlf import DQN
-from rlf.rl.model import MLPBase, TwoLayerMlpWithAction
-import torch.nn as nn
-import torch.nn.functional as F
-from rlf.args import str2bool
-
-
-def reg_init(m):
-    return m
-
-def get_actor_head(hidden_dim, action_dim):
-    return nn.Sequential(
-            nn.Linear(hidden_dim, action_dim),
-            nn.Tanh())
-
-class HerRunSettings(TestRunSettings):
-    def get_policy(self):
-        hidden_size = 256
-        if 'BitFlip' in self.base_args.env_name:
-            return DQN(
-                    get_base_net_fn=lambda i_shape, recurrent: MLPBase(
-                        i_shape[0], False, (hidden_size,),
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    use_goal=True
-                    )
-        else:
-            return RegActorCritic(
-                    get_actor_fn=lambda _, i_shape: MLPBase(
-                        i_shape[0], False, (hidden_size, hidden_size),
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    get_actor_head_fn=get_actor_head,
-                    get_critic_fn=lambda _, i_shape, a_space: TwoLayerMlpWithAction(
-                        i_shape[0], (hidden_size, hidden_size), a_space.shape[0],
-                        weight_init=reg_init,
-                        get_activation=lambda: nn.ReLU()),
-                    get_critic_head_fn = lambda hidden_dim: nn.Linear(hidden_dim, 1),
-                    use_goal=True
-                    )
-
-    def get_algo(self):
-        pass_kwargs = {}
-        if self.base_args.use_her:
-            pass_kwargs['create_storage_buff_fn'] = create_her_storage_buff
-        if 'BitFlip' in self.base_args.env_name:
-            return QLearning(**pass_kwargs)
-        else:
-            return DDPG(**pass_kwargs)
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument('--use-her', default=True, type=str2bool)
-
-if __name__ == "__main__":
-    run_policy(HerRunSettings())
diff --git a/rl-toolkit/examples/test_cmds/her/fetch_reach.cmd b/rl-toolkit/examples/test_cmds/her/fetch_reach.cmd
deleted file mode 100644
index f6736d8..0000000
--- a/rl-toolkit/examples/test_cmds/her/fetch_reach.cmd
+++ /dev/null
@@ -1,3 +0,0 @@
-# 50 epochs
-# Each epoch 38 (processes) * 50 (episodes) * 50 (steps)
-python tests/test_cmds/her/def.py --prefix 'her-test' --env-name "HandReach-v0" --log-smooth-len 1 --save-interval 1 --lr 3e-4 --trans-buffer-size 1e6 --linear-lr-decay False --max-grad-norm -1 --num-processes 38 --update-every 1 --log-interval 1 --eval-interval 1 --gamma 0.98 --normalize-env True --num-env-steps 4.75e6 --num-steps 2500 --tau 0.05 --batch-size 256  --eval-num-processes 10 --num-eval 1 --warmup-step 0 --rnd-prob 0.3
diff --git a/rl-toolkit/examples/test_cmds/her/hand_reach.cmd b/rl-toolkit/examples/test_cmds/her/hand_reach.cmd
deleted file mode 100644
index f6736d8..0000000
--- a/rl-toolkit/examples/test_cmds/her/hand_reach.cmd
+++ /dev/null
@@ -1,3 +0,0 @@
-# 50 epochs
-# Each epoch 38 (processes) * 50 (episodes) * 50 (steps)
-python tests/test_cmds/her/def.py --prefix 'her-test' --env-name "HandReach-v0" --log-smooth-len 1 --save-interval 1 --lr 3e-4 --trans-buffer-size 1e6 --linear-lr-decay False --max-grad-norm -1 --num-processes 38 --update-every 1 --log-interval 1 --eval-interval 1 --gamma 0.98 --normalize-env True --num-env-steps 4.75e6 --num-steps 2500 --tau 0.05 --batch-size 256  --eval-num-processes 10 --num-eval 1 --warmup-step 0 --rnd-prob 0.3
diff --git a/rl-toolkit/examples/test_cmds/ppo/__init__.py b/rl-toolkit/examples/test_cmds/ppo/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/rl-toolkit/examples/test_cmds/ppo/halfcheetah.cmd b/rl-toolkit/examples/test_cmds/ppo/halfcheetah.cmd
deleted file mode 100644
index e0e04c9..0000000
--- a/rl-toolkit/examples/test_cmds/ppo/halfcheetah.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/ppo/main.py --prefix 'ppo-test' --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 10000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --env-name "HalfCheetah-v3" --env-log-dir ~/tmp --eval-interval -1 --log-smooth-len 10
diff --git a/rl-toolkit/examples/test_cmds/ppo/hopper.cmd b/rl-toolkit/examples/test_cmds/ppo/hopper.cmd
deleted file mode 100644
index 039b205..0000000
--- a/rl-toolkit/examples/test_cmds/ppo/hopper.cmd
+++ /dev/null
@@ -1 +0,0 @@
-python tests/test_cmds/ppo/main.py --prefix 'ppo-test' --use-proper-time-limits --linear-lr-decay True --lr 3e-4 --entropy-coef 0 --num-env-steps 3000000 --num-mini-batch 32 --num-epochs 10 --num-steps 64 --env-name "Hopper-v3" --eval-interval -1 --log-smooth-len 10 --save-interval -1
diff --git a/rl-toolkit/examples/test_cmds/ppo/main.py b/rl-toolkit/examples/test_cmds/ppo/main.py
deleted file mode 100644
index b1f326f..0000000
--- a/rl-toolkit/examples/test_cmds/ppo/main.py
+++ /dev/null
@@ -1,16 +0,0 @@
-import sys
-sys.path.insert(0, './')
-from rlf import PPO
-from rlf import run_policy
-from tests.test_run_settings import TestRunSettings
-from rlf.policies.actor_critic.dist_actor_critic import DistActorCritic
-
-class PPORunSettings(TestRunSettings):
-    def get_policy(self):
-        return DistActorCritic()
-
-    def get_algo(self):
-        return PPO()
-
-if __name__ == "__main__":
-    run_policy(PPORunSettings())
diff --git a/rl-toolkit/examples/test_run_settings.py b/rl-toolkit/examples/test_run_settings.py
deleted file mode 100644
index d381756..0000000
--- a/rl-toolkit/examples/test_run_settings.py
+++ /dev/null
@@ -1,30 +0,0 @@
-from rlf.run_settings import RunSettings
-from rlf.rl.loggers.wb_logger import WbLogger
-from rlf.rl.loggers.tb_logger import TbLogger
-from rlf.rl.loggers.plt_logger import PltLogger
-from rlf.rl.loggers.base_logger import BaseLogger
-import os.path as osp
-from rlf.args import str2bool
-
-class TestRunSettings(RunSettings):
-    def get_logger(self):
-        if self.base_args.tb:
-            return TbLogger('./data/tb')
-        elif self.base_args.plt:
-            return PltLogger(["avg_r"], "Steps", "Reward", "Reward Curve")
-        elif self.base_args.no_wb:
-            return BaseLogger()
-        else:
-            return WbLogger()
-
-    def get_config_file(self):
-        # Path to testing config
-        config_dir = osp.dirname(osp.realpath(__file__))
-        return osp.join(config_dir, 'config.yaml')
-
-    def get_add_args(self, parser):
-        parser.add_argument('--no-wb', default=False, action='store_true')
-        parser.add_argument('--tb', default=False, action='store_true')
-        parser.add_argument('--plt', default=False, action='store_true')
-        parser.add_argument('--env-name')
-
diff --git a/rl-toolkit/rlf/algos/__init__.py b/rl-toolkit/rlf/algos/__init__.py
index 0e16122..22acb0d 100644
--- a/rl-toolkit/rlf/algos/__init__.py
+++ b/rl-toolkit/rlf/algos/__init__.py
@@ -6,9 +6,6 @@ from rlf.algos.off_policy.q_learning import QLearning
 from rlf.algos.off_policy.sac import SAC
 from rlf.algos.on_policy.sarsa import SARSA
 from rlf.algos.il.base_il import BaseILAlgo
-from rlf.algos.il.gail import GAIL
-from rlf.algos.il.gaifo import GAIFO
-from rlf.algos.il.gail import GailDiscrim
 from rlf.algos.il.bc import BehavioralCloning
 from rlf.algos.il.dbc import DBC
 from rlf.algos.il.diff_policy import DiffPolicy
@@ -16,7 +13,5 @@ from rlf.algos.il.ibc import IBC
 from rlf.algos.il.ae_bc import Ae_bc
 from rlf.algos.il.eng_bc import Eng_bc
 from rlf.algos.il.gan_bc import GANBC
-from rlf.algos.il.bco import BehavioralCloningFromObs
-from rlf.algos.il.bc_pretrain import BehavioralCloningPretrain
 from rlf.algos.nested_algo import NestedAlgo
 from rlf.algos.il.base_irl import BaseIRLAlgo
diff --git a/rl-toolkit/rlf/algos/il/ae_bc.py b/rl-toolkit/rlf/algos/il/ae_bc.py
index 6a586d0..56998c2 100644
--- a/rl-toolkit/rlf/algos/il/ae_bc.py
+++ b/rl-toolkit/rlf/algos/il/ae_bc.py
@@ -247,5 +247,4 @@ class Ae_bc(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
diff --git a/rl-toolkit/rlf/algos/il/bc.py b/rl-toolkit/rlf/algos/il/bc.py
index 273abcf..02fb985 100644
--- a/rl-toolkit/rlf/algos/il/bc.py
+++ b/rl-toolkit/rlf/algos/il/bc.py
@@ -103,10 +103,6 @@ class BehavioralCloning(BaseILAlgo):
         # get predicted randome noise at time t
         output = model(x, t.squeeze(-1).to(self.args.device))
         output2 = model(x2, t.squeeze(-1).to(self.args.device))
-        # print(f"output: {output}")
-        # # input()
-        # print(f"output2: {output2}")
-        # input()
         
         # calculate the loss between actual noise and predicted noise
         loss = (e - output).square().mean()
@@ -299,7 +295,6 @@ class BehavioralCloning(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
         parser.add_argument('--num-units', type=int, default=128) #hidden dim of ddpm
         parser.add_argument('--ddpm-depth', type=int, default=4)
diff --git a/rl-toolkit/rlf/algos/il/bc_pretrain.py b/rl-toolkit/rlf/algos/il/bc_pretrain.py
deleted file mode 100644
index f545212..0000000
--- a/rl-toolkit/rlf/algos/il/bc_pretrain.py
+++ /dev/null
@@ -1,65 +0,0 @@
-from rlf.algos.nested_algo import NestedAlgo
-from rlf.algos.il.bc import BehavioralCloning
-import rlf.rl.utils as rutils
-import copy
-import torch
-
-class BehavioralCloningPretrain(NestedAlgo):
-    def __init__(self, modules, designated_rl_idx, designated_settings_idx=0):
-        self.bc = BehavioralCloning(set_arg_defs=False)
-        super().__init__(modules, designated_rl_idx, designated_settings_idx)
-
-    def init(self, policy, args):
-        self.args = args
-        self.policy = policy
-        super().init(policy, args)
-        self.bc.init(policy, args)
-
-    def first_train(self, log, eval_policy, env_interface):
-        rutils.pstart_sep()
-        print('Pre-training policy with BC')
-        self.bc.full_train()
-
-        bc_eval_args = copy.copy(self.args)
-        bc_eval_args.eval_num_processes = 32
-        bc_eval_args.num_eval = 5
-        bc_eval_args.num_render = 0
-        tmp_env = eval_policy(self.policy, 0, bc_eval_args)
-        if tmp_env is not None:
-            tmp_env.close()
-
-        rutils.pend_sep()
-
-        super().first_train(log, eval_policy, env_interface)
-
-    def get_add_args(self, parser):
-        self.bc.get_add_args(parser)
-        super().get_add_args(parser)
-
-    def get_env_settings(self, args):
-        settings = super().get_env_settings(args)
-        self.bc_env_settings = self.bc.get_env_settings(args)
-        #settings.state_fn = self.bc_env_settings.state_fn
-        return settings
-
-    def set_env_ref(self, envs):
-        super().set_env_ref(envs)
-
-        raise ValueError("Env ref code is depricated, please see for how to fix")
-        #if env_norm is not None and self.bc_env_settings.state_fn is not None \
-        #        and env_norm.ob_rms_dict is not None:
-        #    # Set the normalization of the environment for further RL training.
-        #    #state_stats = self.bc.expert_stats['state']
-        #    if None in env_norm.ob_rms_dict:
-        #        ob_rms = env_norm.ob_rms_dict[None]
-        #    else:
-        #        ob_rms = env_norm.ob_rms_dict['observation']
-        #    ob_rms.mean = self.bc.norm_mean.cpu().float().numpy()
-        #    # Conversion from std to var
-        #    ob_rms.var = self.bc.norm_var.cpu().float().numpy()
-        #    ob_rms.count += len(self.bc.expert_dataset)
-
-    def update(self, storage):
-        return super().update(storage)
-
-
diff --git a/rl-toolkit/rlf/algos/il/bco.py b/rl-toolkit/rlf/algos/il/bco.py
deleted file mode 100644
index cfaa619..0000000
--- a/rl-toolkit/rlf/algos/il/bco.py
+++ /dev/null
@@ -1,450 +0,0 @@
-import os
-import os.path as osp
-from functools import partial
-
-import numpy as np
-import rlf.algos.utils as autils
-import rlf.rl.utils as rutils
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from gym import spaces
-from rlf.algos.base_algo import BaseAlgo
-from rlf.algos.il.bc import BehavioralCloning
-from rlf.policies.random_policy import RandomPolicy
-from rlf.rl.envs import make_vec_envs_easy
-from rlf.rl.model import Flatten
-from rlf.storage.rollout_storage import RolloutStorage
-from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler
-from tqdm import tqdm
-
-
-class InvFunc(nn.Module):
-    def __init__(self, get_state_enc, obs_shape, action_size, hidden_dim=64):
-        super().__init__()
-        self.is_img = len(obs_shape) == 3
-
-        if self.is_img:
-            n = obs_shape[1]
-            m = obs_shape[2]
-            image_embedding_size = ((n - 1) // 2 - 2) * ((m - 1) // 2 - 2) * 64
-            self.head = nn.Sequential(
-                nn.Conv2d(2 * obs_shape[0], 16, (2, 2)),
-                nn.ReLU(),
-                nn.MaxPool2d((2, 2)),
-                nn.Conv2d(16, 32, (2, 2)),
-                nn.ReLU(),
-                nn.Conv2d(32, 64, (2, 2)),
-                nn.ReLU(),
-                Flatten(),
-                nn.Linear(image_embedding_size, hidden_dim),
-                nn.ReLU(),
-                nn.Linear(hidden_dim, hidden_dim),
-                nn.ReLU(),
-                nn.Linear(hidden_dim, action_size),
-            )
-        else:
-            self.head = nn.Sequential(
-                nn.Linear(2 * obs_shape[0], hidden_dim),
-                nn.ReLU(),
-                nn.Linear(hidden_dim, hidden_dim),
-                nn.ReLU(),
-                nn.Linear(hidden_dim, action_size),
-            )
-
-    def forward(self, state_1, state_2):
-        if self.is_img:
-            x = torch.cat([state_1, state_2], dim=1)
-        else:
-            x = torch.cat([state_1, state_2], dim=-1)
-
-        tmp = self.head(x)
-        return tmp
-
-
-def select_batch(trans_idx, dataset, device, ob_shape):
-    """
-    - trans_idx (list(int)): indices to select from dataset.
-    - dataset (list(dict)): contains a list of dictionaries with the state
-      action data.
-    """
-    use_state_0 = []
-    use_state_1 = []
-    use_action = []
-    for i in trans_idx:
-        use_state_0.append(dataset[i]["s0"])
-        use_state_1.append(dataset[i]["s1"])
-        use_action.append(dataset[i]["action"])
-    use_state_0 = torch.stack(use_state_0).to(device).view(-1, *ob_shape)
-    use_state_1 = torch.stack(use_state_1).to(device).view(-1, *ob_shape)
-    true_action = torch.stack(use_action).to(device).squeeze(1)
-    return use_state_0, use_state_1, true_action
-
-
-class BehavioralCloningFromObs(BehavioralCloning):
-    def init(self, policy, args):
-        if args.bco_alpha != 0:
-            # Adjust the amount of online experience based on args.
-            args.num_env_steps = args.bco_alpha_size * args.bco_alpha
-            args.num_steps = args.bco_alpha_size // args.num_processes
-            print(f"Adjusted # steps to {args.num_steps}")
-            print(f"Adjusted # env interactions to {args.num_env_steps}")
-
-        super().init(policy, args)
-
-        if self._arg("lr_env_steps") is None and args.bco_alpha != 0:
-            # Adjust the learning rate decay steps based on the number of BC
-            # updates performed.
-            bc_updates = super().get_num_updates()
-            bco_full_updates = self.get_num_updates() + 1
-            # We perform a full BC update for each "BCO update". "BCO updates"
-            # come from the initial training and for each update from online
-            # experience determined according to alpha.
-            self.lr_updates = bc_updates * bco_full_updates
-            print(f"Adjusted # lr updates to {self.lr_updates}")
-
-        get_state_enc = partial(
-            self.policy.get_base_net_fn, rutils.get_obs_shape(self.policy.obs_space)
-        )
-        self.inv_func = InvFunc(
-            get_state_enc,
-            rutils.get_obs_shape(self.policy.obs_space),
-            self.action_dim,
-        )
-        self.inv_func = self.inv_func.to(self.args.device)
-        self.inv_opt = optim.Adam(self.inv_func.parameters(), lr=self.args.bco_inv_lr)
-
-    def set_env_ref(self, envs):
-        super().set_env_ref(envs)
-        self.use_envs = envs
-
-    def first_train(self, log, eval_policy, env_interface):
-        """
-        Gathers the random experience and trains the inverse model on it.
-        """
-        n_steps = self.args.bco_expl_steps // self.args.num_processes
-        base_data_dir = "data/traj/bco"
-        if not osp.exists(base_data_dir):
-            os.makedirs(base_data_dir)
-
-        loaded_traj = None
-        if self.args.bco_expl_load is not None:
-            load_path = osp.join(base_data_dir, self.args.bco_expl_load)
-            if osp.exists(load_path) and not self.args.bco_expl_refresh:
-                loaded_traj = torch.load(load_path)
-                states = loaded_traj["states"]
-                actions = loaded_traj["actions"]
-                dones = loaded_traj["dones"]
-                print(f"Loaded expl trajectories from {load_path}")
-
-        if loaded_traj is None:
-            policy = RandomPolicy()
-            policy.init(
-                self.use_envs.observation_space, self.use_envs.action_space, self.args
-            )
-            rutils.pstart_sep()
-            print("Collecting exploration experience")
-            states = []
-            actions = []
-            state = rutils.get_def_obs(self.use_envs.reset())
-            states.extend(state)
-            dones = [True]
-            for _ in tqdm(range(n_steps)):
-                ac_info = policy.get_action(state, None, None, None, None)
-                state, reward, done, info = self.use_envs.step(ac_info.take_action)
-                state = rutils.get_def_obs(state)
-                actions.extend(ac_info.action)
-                dones.extend(done)
-                states.extend(state)
-            rutils.pend_sep()
-            self.use_envs.reset()
-
-        if self.args.bco_expl_load is not None and loaded_traj is None:
-            # Save the data.
-            torch.save(
-                {
-                    "states": states,
-                    "actions": actions,
-                    "dones": dones,
-                },
-                load_path,
-            )
-            print(f"Saved data to {load_path}")
-
-        if self.args.bco_inv_load is not None:
-            self.inv_func.load_state_dict(torch.load(self.args.bco_inv_load))
-
-        self._update_all(states, actions, dones)
-
-    def _train_inv_func(self, trans_sampler, dataset):
-        infer_ac_losses = []
-        for i in tqdm(range(self.args.bco_inv_epochs)):
-            for trans_idx in trans_sampler:
-                use_state_0, use_state_1, true_action = select_batch(
-                    trans_idx,
-                    dataset,
-                    self.args.device,
-                    rutils.get_obs_shape(self.policy.obs_space),
-                )
-                pred_action = self.inv_func(use_state_0, use_state_1)
-                loss = autils.compute_ac_loss(
-                    pred_action,
-                    true_action.view(-1, self.action_dim),
-                    self.policy.action_space,
-                )
-                infer_ac_losses.append(loss.item())
-
-                self.inv_opt.zero_grad()
-                loss.backward()
-                self.inv_opt.step()
-        return infer_ac_losses
-
-    def _infer_inv_accuracy(self, trans_sampler, dataset):
-        total_count = 0
-        num_correct = 0
-        with torch.no_grad():
-            for trans_idx in trans_sampler:
-                use_state_0, use_state_1, true_action = select_batch(
-                    trans_idx,
-                    dataset,
-                    self.args.device,
-                    rutils.get_obs_shape(self.policy.obs_space),
-                )
-                pred_action = self.inv_func(use_state_0, use_state_1)
-                pred_class = torch.argmax(pred_action, dim=-1)
-                num_correct += (pred_class == true_action.view(-1)).float().sum()
-                total_count += float(use_state_0.shape[0])
-        return 100.0 * (num_correct / total_count)
-
-    def _update_all(self, states, actions, dones):
-        """
-        - states (list[N+1])
-        - masks (list[N+1])
-        - actions (list[N])
-        Performs a complete update of the model by following these steps:
-            1. Train inverse function with ground truth data provided.
-            2. Infer actions in expert dataset
-            3. Train BC
-        """
-        dataset = [
-            {"s0": states[i], "s1": states[i + 1], "action": actions[i]}
-            for i in range(len(actions))
-            if not dones[i + 1]
-        ]
-
-        rutils.pstart_sep()
-        print(f"BCO Update {self.update_i}/{self.args.bco_alpha}")
-        print("---")
-
-        print("Training inverse function")
-        dataset_idxs = list(range(len(dataset)))
-        np.random.shuffle(dataset_idxs)
-
-        eval_len = int(len(dataset_idxs) * self.args.bco_inv_eval_holdout)
-        if eval_len != 0.0:
-            train_trans_sampler = BatchSampler(
-                SubsetRandomSampler(dataset_idxs[:-eval_len]),
-                self.args.bco_inv_batch_size,
-                drop_last=False,
-            )
-            val_trans_sampler = BatchSampler(
-                SubsetRandomSampler(dataset_idxs[-eval_len:]),
-                self.args.bco_inv_batch_size,
-                drop_last=False,
-            )
-        else:
-            train_trans_sampler = BatchSampler(
-                SubsetRandomSampler(dataset_idxs),
-                self.args.bco_inv_batch_size,
-                drop_last=False,
-            )
-
-        if self.args.bco_inv_load is None or self.update_i > 0:
-            infer_ac_losses = self._train_inv_func(train_trans_sampler, dataset)
-            rutils.plot_line(
-                infer_ac_losses,
-                f"ac_inv_loss_{self.update_i}.png",
-                self.args.vid_dir,
-                not self.args.no_wb,
-                self.get_completed_update_steps(self.update_i),
-            )
-            if self.update_i == 0:
-                # Only save the inverse model on the first epoch for debugging
-                # purposes
-                rutils.save_model(
-                    self.inv_func, f"inv_func_{self.update_i}.pt", self.args
-                )
-
-        if eval_len != 0.0:
-            if not isinstance(self.policy.action_space, spaces.Discrete):
-                raise ValueError(
-                    (
-                        "Evaluating the holdout accuracy is only",
-                        " supported for discrete action spaces right now",
-                    )
-                )
-            accuracy = self._infer_inv_accuracy(val_trans_sampler, dataset)
-            print("Inferred actions with %.2f accuracy" % accuracy)
-
-        if isinstance(self.expert_dataset, torch.utils.data.Subset):
-            s0 = self.expert_dataset.dataset.trajs["obs"].to(self.args.device).float()
-            s1 = (
-                self.expert_dataset.dataset.trajs["next_obs"]
-                .to(self.args.device)
-                .float()
-            )
-            dataset_device = self.expert_dataset.dataset.trajs["obs"].device
-        else:
-            s0 = self.expert_dataset.trajs["obs"].to(self.args.device).float()
-            s1 = self.expert_dataset.trajs["next_obs"].to(self.args.device).float()
-            dataset_device = self.expert_dataset.trajs["obs"].device
-
-        # Perform inference on the expert states
-        with torch.no_grad():
-            pred_actions = self.inv_func(s0, s1).to(dataset_device)
-            pred_actions = rutils.get_ac_compact(self.policy.action_space, pred_actions)
-            if not self.args.bco_oracle_actions:
-                if isinstance(self.expert_dataset, torch.utils.data.Subset):
-                    self.expert_dataset.dataset.trajs["actions"] = pred_actions
-                else:
-                    self.expert_dataset.trajs["actions"] = pred_actions
-        # Recreate the dataset for BC training so we can be sure it has the
-        # most recent data.
-        self._create_train_loader(self.args)
-
-        print("Training Policy")
-        self.full_train(self.update_i)
-        self.update_i += 1
-        rutils.pend_sep()
-
-    def get_num_updates(self):
-        if self.args.bco_alpha == 0:
-            return 0
-        return BaseAlgo.get_num_updates(self)
-
-    def get_completed_update_steps(self, num_updates):
-        return BaseAlgo.get_completed_update_steps(self, num_updates)
-
-    def update(self, storage):
-        masks = storage.masks.view(-1, 1)
-        actions = storage.actions.view(-1, storage.actions.shape[-1])
-        obs = storage.get_def_obs_seq()
-        obs = obs.view(-1, *obs.shape[2:])
-        # Update based on collected experience from environment
-        dones = [(not bool(x)) for x in masks]
-        self._update_all(obs, actions, dones)
-        return {}
-
-    def get_storage_buffer(self, policy, envs, args):
-        # Rollout buffer to store the collected online experience
-        return RolloutStorage(
-            args.num_steps,
-            args.num_processes,
-            envs.observation_space,
-            envs.action_space,
-            args,
-        )
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        parser.add_argument(
-            "--bco-expl-steps",
-            type=int,
-            default=1000,
-            help="""
-                Number of random exploration steps.
-                """,
-        )
-
-        # Inverse model learning arguments.
-        parser.add_argument(
-            "--bco-inv-lr",
-            type=float,
-            default=0.001,
-            help="""
-                The learning rate of the action inverse model
-                """,
-        )
-        parser.add_argument(
-            "--bco-inv-epochs",
-            type=int,
-            default=1,
-            help="""
-                The number of epochs when training the inverse model. This is
-                used both for the alpha updates and the pre-alpha updates.
-                """,
-        )
-        parser.add_argument(
-            "--bco-inv-eval-holdout",
-            type=float,
-            default=0.0,
-            help="""
-                The fraction of data that should be withheld when training the
-                inverse model and later used for evaluation.
-                """,
-        )
-        parser.add_argument(
-            "--bco-inv-batch-size",
-            type=int,
-            default=32,
-            help="""
-                The batch size for the inverse action model training.
-                """,
-        )
-
-        # Online learning arguments.
-        parser.add_argument(
-            "--bco-alpha",
-            type=int,
-            default=0,
-            help="""
-                Number of online updates.
-                """,
-        )
-        parser.add_argument(
-            "--bco-alpha-size",
-            type=int,
-            default=1000,
-            help="""
-                Size of each online update.
-                """,
-        )
-        parser.add_argument(
-            "--bco-oracle-actions",
-            action="store_true",
-            help="""
-                Use the ground truth expert actions rather than the inferred
-                ones. FOR DEBUGGING ONLY. This does not use the learned inverse
-                model.
-                """,
-        )
-
-        # Saving files to speed things up arguments
-        parser.add_argument(
-            "--bco-expl-load",
-            type=str,
-            default=None,
-            help="""
-                File to load/save the random explorations. If the file exists
-                the random explorations will be loaded from here. If it does
-                not exist the random explorations will be saved here.
-                """,
-        )
-        parser.add_argument(
-            "--bco-expl-refresh",
-            action="store_true",
-            help="""
-                Regenerate the random explorations even if the expl-load file
-                is specified and exists.
-                """,
-        )
-        parser.add_argument(
-            "--bco-inv-load",
-            type=str,
-            default=None,
-            help="""
-                If specified the inverse model will be loaded from here and
-                not trained on the random exploration phase. However, it
-                **will** be trained on all subsequent alpha updates.
-                """,
-        )
diff --git a/rl-toolkit/rlf/algos/il/dbc.py b/rl-toolkit/rlf/algos/il/dbc.py
index aed8cdd..2f745b6 100644
--- a/rl-toolkit/rlf/algos/il/dbc.py
+++ b/rl-toolkit/rlf/algos/il/dbc.py
@@ -402,7 +402,6 @@ class DBC(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
         parser.add_argument('--fix-step-when-get-loss', type=str2bool, default=False)
         parser.add_argument('--fix-range-when-get-loss', type=str2bool, default=False)
diff --git a/rl-toolkit/rlf/algos/il/diff_policy.py b/rl-toolkit/rlf/algos/il/diff_policy.py
index c3c0490..3a7c623 100644
--- a/rl-toolkit/rlf/algos/il/diff_policy.py
+++ b/rl-toolkit/rlf/algos/il/diff_policy.py
@@ -200,7 +200,6 @@ class DiffPolicy(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
         parser.add_argument("--lambda-bc", type=float, default=1)
         parser.add_argument("--lambda-dm", type=float, default=1)
diff --git a/rl-toolkit/rlf/algos/il/eng_bc.py b/rl-toolkit/rlf/algos/il/eng_bc.py
index cbb99e5..e9e9d6d 100644
--- a/rl-toolkit/rlf/algos/il/eng_bc.py
+++ b/rl-toolkit/rlf/algos/il/eng_bc.py
@@ -222,6 +222,5 @@ class Eng_bc(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
         parser.add_argument("--model-path", type=str, default=None)
diff --git a/rl-toolkit/rlf/algos/il/gaifo.py b/rl-toolkit/rlf/algos/il/gaifo.py
deleted file mode 100644
index 5606578..0000000
--- a/rl-toolkit/rlf/algos/il/gaifo.py
+++ /dev/null
@@ -1,184 +0,0 @@
-from rlf.algos.il.gail import GailDiscrim
-import torch
-import torch.nn as nn
-from functools import partial
-import rlf.il.utils as iutils
-from rlf.algos.nested_algo import NestedAlgo
-from rlf.algos.on_policy.ppo import PPO
-from rlf.il.transition_dataset import TransitionDataset
-import rlf.rl.utils as rutils
-from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler
-from rlf.rl.model import ConcatLayer
-from collections import defaultdict
-import torch.nn.functional as F
-
-class GAIFO(NestedAlgo):
-    def __init__(self, agent_updater=PPO(), get_discrim=None):
-        super().__init__([GaifoDiscrim(get_discrim), agent_updater], 1)
-
-class PairTransitionDataset(TransitionDataset):
-    def __init__(self, load_path, transform_dem_dataset_fn):
-        super().__init__(load_path, transform_dem_dataset_fn)
-        self.trajs['next_obs'] = self.trajs['next_obs'].float()
-
-    def __getitem__(self, i):
-        return {
-                'state': self.trajs['obs'][i],
-                'next_state': self.trajs['next_obs'][i],
-                'done': self.trajs['done'][i],
-                }
-
-def get_default_discrim(ac_dim, in_shape):
-    """
-    Returns: (nn.Module) Should take two states as input.
-    """
-    hidden_dim = 64
-    return nn.Sequential(
-        ConcatLayer(-1),
-        nn.Linear(2*in_shape[0], hidden_dim), nn.Tanh(),
-        nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),
-        nn.Linear(hidden_dim, 1))
-
-class DoubleStateDiscrim(nn.Module):
-    def __init__(self, state_enc, hidden_dim=64):
-        super().__init__()
-        self.state_enc = state_enc
-        output_size = self.state_enc.output_shape[0]
-        self.head = nn.Sequential(
-                nn.Linear(output_size, hidden_dim),
-                nn.Tanh(),
-                nn.Linear(hidden_dim, hidden_dim),
-                nn.Tanh(),
-                nn.Linear(hidden_dim, 1))
-
-    def forward(self, s0, s1):
-        both_s = torch.cat([s0, s1], dim=1)
-        both_s_enc, _ = self.state_enc(both_s, None, None)
-        return self.head(both_s_enc)
-
-
-class GaifoDiscrim(GailDiscrim):
-    def _create_discrim(self):
-        new_shape = list(rutils.get_obs_shape(self.policy.obs_space))
-        new_shape[0] *= 2
-        base_net = self.policy.get_base_net_fn(new_shape)
-        return DoubleStateDiscrim(base_net).to(self.args.device)
-
-    def _get_traj_dataset(self, traj_load_path):
-        return PairTransitionDataset(traj_load_path, self._transform_dem_dataset_fn)
-
-    def _trans_batches(self, expert_batch, agent_batch):
-        agent_batch = iutils.select_idx_from_dict(agent_batch,
-                self.agent_obs_pairs)
-        return expert_batch, agent_batch
-
-    def _compute_discrim_loss(self, agent_batch, expert_batch, obsfilt):
-        d = self.args.device
-        exp_s0 = self._norm_expert_state(expert_batch['state'],
-                obsfilt).float()
-        exp_s1 = self._norm_expert_state(expert_batch['next_state'],
-                obsfilt).float()
-
-        agent_s0 = agent_batch['state'].to(d)
-        agent_s1 = agent_batch['next_state'].to(d)
-
-        expert_d = self.discrim_net(exp_s0, exp_s1)
-        agent_d = self.discrim_net(agent_s0, agent_s1)
-        # Select the valid transitions where the episode did not end in
-        # between.
-        expert_d = expert_d[expert_batch['done'] == 0]
-        agent_d = agent_d[agent_batch['mask'] == 1]
-        return expert_d, agent_d, 0
-
-    def _compute_disc_val(self, state, next_state, action):
-        return self.discrim_net(state, next_state)
-
-    def _get_sampler(self, storage):
-        obs = storage.get_def_obs_seq()
-        ob_shape = rutils.get_obs_shape(self.policy.obs_space)
-        self.agent_obs_pairs = {
-                'state': obs[:-1].view(-1, *ob_shape),
-                'next_state': obs[1:].view(-1, *ob_shape),
-                'mask': storage.masks[:-1].view(-1, 1),
-                }
-        failure_sampler = BatchSampler(SubsetRandomSampler(
-            range(self.args.num_steps)), self.args.traj_batch_size,
-            drop_last=True)
-        return self.expert_train_loader, failure_sampler
-
-    def get_env_settings(self, args):
-        settings = super().get_env_settings(args)
-        settings.include_info_keys.extend([
-            ('final_obs', lambda env: rutils.get_obs_shape(env.observation_space))
-            ])
-        return settings
-
-    def _compute_discrim_reward(self, storage, step, add_info):
-        state = rutils.get_def_obs(storage.get_obs(step))
-
-        next_state = rutils.get_def_obs(storage.get_obs(step+1))
-        masks = storage.masks[step+1]
-        finished_episodes = [i for i in range(len(masks)) if masks[i] == 0.0]
-        add_inputs = {k: v[(step+1)-1] for k,v in add_info.items()}
-        obsfilt = self.get_env_ob_filt()
-        for i in finished_episodes:
-            next_state[i] = add_inputs['final_obs'][i]
-            if obsfilt is not None:
-                next_state[i] = torch.FloatTensor(obsfilt(next_state[i].cpu().numpy(),
-                        update=False)).to(self.args.device)
-
-        d_val = self.discrim_net(state, next_state)
-        s = torch.sigmoid(d_val)
-        eps = 1e-20
-        if self.args.reward_type == 'airl':
-            reward = (s + eps).log() - (1 - s + eps).log()
-        elif self.args.reward_type == 'gail':
-            reward = (s + eps).log()
-        elif self.args.reward_type == 'raw':
-            reward = d_val
-        elif self.args.reward_type == 'gaifo':
-            reward = -1.0 * (s + eps).log()
-        else:
-            raise ValueError(f"Unrecognized reward type {self.args.reward_type}")
-        return reward
-
-    def _update_reward_func(self, storage):
-        self.discrim_net.train()
-
-        d = self.args.device
-        log_vals = defaultdict(lambda: 0)
-        obsfilt = self.get_env_ob_filt()
-
-        n = 0
-        expert_sampler, agent_sampler = self._get_sampler(storage)
-        for epoch_i in range(self.args.n_gail_epochs):
-            for expert_batch, agent_batch in zip(expert_sampler, agent_sampler):
-                expert_batch, agent_batch = self._trans_batches(
-                    expert_batch, agent_batch)
-                n += 1
-                expert_d, agent_d, grad_pen = self._compute_discrim_loss(agent_batch, expert_batch,
-                        obsfilt)
-                if self.args.reward_type == 'gaifo':
-                    expert_loss = F.binary_cross_entropy_with_logits(expert_d,
-                                                                     torch.zeros(expert_d.shape).to(d))
-                    agent_loss = F.binary_cross_entropy_with_logits(agent_d,
-                                                                    torch.ones(agent_d.shape).to(d))
-                else:
-                    expert_loss = F.binary_cross_entropy_with_logits(expert_d,
-                                                                     torch.ones(expert_d.shape).to(d))
-                    agent_loss = F.binary_cross_entropy_with_logits(agent_d,
-                                                                    torch.zeros(agent_d.shape).to(d))
-                discrim_loss = expert_loss + agent_loss
-
-                self.opt.zero_grad()
-                discrim_loss.backward()
-                self.opt.step()
-
-                log_vals['discrim_loss'] += discrim_loss.item()
-                log_vals['expert_loss'] += expert_loss.item()
-                log_vals['agent_loss'] += agent_loss.item()
-
-        for k in log_vals:
-            log_vals[k] /= n
-
-        return log_vals
diff --git a/rl-toolkit/rlf/algos/il/gail.py b/rl-toolkit/rlf/algos/il/gail.py
deleted file mode 100644
index 84ef26e..0000000
--- a/rl-toolkit/rlf/algos/il/gail.py
+++ /dev/null
@@ -1,264 +0,0 @@
-from rlf.algos.il.base_irl import BaseIRLAlgo
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import rlf.rl.utils as rutils
-import rlf.algos.utils as autils
-from collections import defaultdict
-from rlf.baselines.common.running_mean_std import RunningMeanStd
-from rlf.algos.nested_algo import NestedAlgo
-from rlf.algos.on_policy.ppo import PPO
-from rlf.args import str2bool
-import torch.optim as optim
-import numpy as np
-from rlf.rl.model import ConcatLayer
-from rlf.rl.model import InjectNet
-from functools import partial
-from rlf.exp_mgr.viz_utils import append_text_to_image
-
-
-def get_default_discrim():
-    """
-    - ac_dim: int will be 0 if no action are used.
-    Returns: (nn.Module) Should take state AND actions as input if ac_dim
-    != 0. If ac_dim = 0 (discriminator does not use actions) then ONLY take
-    state as input.
-    """
-    hidden_dim = 64
-    layers = [
-            #nn.Linear(in_shape[0] + ac_dim, hidden_dim), nn.Tanh(),
-            nn.Linear(hidden_dim, hidden_dim), nn.Tanh(),
-            nn.Linear(hidden_dim, 1)
-            ]
-
-    return nn.Sequential(*layers), hidden_dim
-
-class GAIL(NestedAlgo):
-    def __init__(self, agent_updater=PPO(), get_discrim=None):
-        super().__init__([GailDiscrim(get_discrim), agent_updater], 1)
-
-class GailDiscrim(BaseIRLAlgo):
-    def __init__(self, get_discrim=None):
-        super().__init__()
-        if get_discrim is None:
-            get_discrim = get_default_discrim
-        self.get_discrim = get_discrim
-
-    def _create_discrim(self):
-        ob_shape = rutils.get_obs_shape(self.policy.obs_space)
-        ac_dim = rutils.get_ac_dim(self.action_space)
-        base_net = self.policy.get_base_net_fn(ob_shape)
-        discrim, dhidden_dim = self.get_discrim()
-        discrim_head = InjectNet(
-            base_net.net,
-            discrim,
-            base_net.output_shape[0], dhidden_dim, ac_dim,
-            self.args.action_input)
-
-        return discrim_head.to(self.args.device)
-
-    def init(self, policy, args):
-        super().init(policy, args)
-        self.action_space = self.policy.action_space
-
-        self.discrim_net = self._create_discrim()
-
-        self.returns = None
-        self.ret_rms = RunningMeanStd(shape=())
-
-        self.opt = optim.Adam(
-            self.discrim_net.parameters(), lr=self.args.disc_lr)
-
-    def _get_sampler(self, storage):
-        agent_experience = storage.get_generator(None,
-                                                 mini_batch_size=self.expert_train_loader.batch_size)
-        return self.expert_train_loader, agent_experience
-
-    def _trans_batches(self, expert_batch, agent_batch):
-        return expert_batch, agent_batch
-
-    def get_env_settings(self, args):
-        settings = super().get_env_settings(args)
-        if not args.gail_state_norm:
-            settings.ret_raw_obs = True
-        settings.mod_render_frames_fn = self.mod_render_frames
-        return settings
-
-    def mod_render_frames(self, frame, env_cur_obs, env_cur_action, env_cur_reward,
-            env_next_obs, **kwargs):
-        use_cur_obs = rutils.get_def_obs(env_cur_obs)
-        use_cur_obs = torch.FloatTensor(use_cur_obs).unsqueeze(0).to(self.args.device)
-
-        if env_cur_action is not None:
-            use_action = torch.FloatTensor(env_cur_action).unsqueeze(0).to(self.args.device)
-            disc_val = self._compute_disc_val(use_cur_obs, use_action).item()
-        else:
-            disc_val = 0.0
-
-        frame = append_text_to_image(frame, [
-            "Discrim: %.3f" % disc_val,
-            "Reward: %.3f" % (env_cur_reward if env_cur_reward is not None else 0.0)
-            ])
-        return frame
-
-    def _norm_expert_state(self, state, obsfilt):
-        if not self.args.gail_state_norm:
-            return state
-        state = state.cpu().numpy()
-
-        if obsfilt is not None:
-            state = obsfilt(state, update=False)
-        state = torch.tensor(state).to(self.args.device)
-        return state
-
-    def _trans_agent_state(self, state, other_state=None):
-        if not self.args.gail_state_norm:
-            if other_state is None:
-                return state['raw_obs']
-            return other_state['raw_obs']
-        return rutils.get_def_obs(state)
-
-    def _compute_discrim_loss(self, agent_batch, expert_batch, obsfilt):
-        expert_actions = expert_batch['actions'].to(self.args.device)
-        expert_actions = self._adjust_action(expert_actions)
-        expert_states = self._norm_expert_state(expert_batch['state'],
-                obsfilt)
-
-        agent_states = self._trans_agent_state(agent_batch['state'],
-                agent_batch['other_state'] if 'other_state' in agent_batch else None)
-        agent_actions = agent_batch['action']
-
-        agent_actions = rutils.get_ac_repr(
-            self.action_space, agent_actions)
-        expert_actions = rutils.get_ac_repr(
-            self.action_space, expert_actions)
-
-        expert_d = self._compute_disc_val(expert_states, expert_actions)
-        agent_d = self._compute_disc_val(agent_states, agent_actions)
-
-        grad_pen = self.compute_pen(expert_states, expert_actions, agent_states,
-                agent_actions)
-
-        return expert_d, agent_d, grad_pen
-
-    def compute_pen(self, expert_states, expert_actions, agent_states, agent_actions):
-        grad_pen = self.args.disc_grad_pen * autils.wass_grad_pen(expert_states,
-                expert_actions, agent_states, agent_actions,
-                self.args.action_input, self._compute_disc_val)
-        return grad_pen
-
-    def _compute_disc_val(self, state, action):
-        return self.discrim_net(state, action)
-
-    def _compute_expert_loss(self, expert_d, expert_batch):
-        return F.binary_cross_entropy_with_logits(expert_d,
-                torch.ones(expert_d.shape).to(self.args.device))
-
-    def _compute_agent_loss(self, agent_d, agent_batch):
-        return F.binary_cross_entropy_with_logits(agent_d,
-                torch.zeros(agent_d.shape).to(self.args.device))
-
-    def _update_reward_func(self, storage):
-        self.discrim_net.train()
-
-        log_vals = defaultdict(lambda: 0)
-        obsfilt = self.get_env_ob_filt()
-
-        n = 0
-        expert_sampler, agent_sampler = self._get_sampler(storage)
-        if agent_sampler is None:
-            # algo requested not to update this step
-            return {}
-        for epoch_i in range(self.args.n_gail_epochs):
-        # for epoch_i in range(5):
-            for expert_batch, agent_batch in zip(expert_sampler, agent_sampler):
-                expert_batch, agent_batch = self._trans_batches(
-                    expert_batch, agent_batch)
-                n += 1
-                expert_d, agent_d, grad_pen = self._compute_discrim_loss(agent_batch, expert_batch,
-                        obsfilt)
-                expert_loss = self._compute_expert_loss(expert_d, expert_batch)
-                agent_loss = self._compute_agent_loss(agent_d, agent_batch)
-
-                discrim_loss = expert_loss + agent_loss
-
-                if self.args.disc_grad_pen != 0.0:
-                    log_vals['grad_pen'] += grad_pen.item()
-                    total_loss = discrim_loss + grad_pen
-                else:
-                    total_loss = discrim_loss
-
-                self.opt.zero_grad()
-                total_loss.backward()
-                self.opt.step()
-
-                log_vals['discrim_loss'] += discrim_loss.item()
-                log_vals['expert_loss'] += expert_loss.item()
-                log_vals['agent_loss'] += agent_loss.item()
-
-        for k in log_vals:
-            log_vals[k] /= n
-
-        return log_vals
-
-    def _compute_discrim_reward(self, storage, step, add_info):
-        state = self._trans_agent_state(storage.get_obs(step))
-        action = storage.actions[step]
-        action = rutils.get_ac_repr(self.action_space, action)
-        d_val = self._compute_disc_val(state, action)
-        s = torch.sigmoid(d_val)
-        eps = 1e-20
-        if self.args.reward_type == 'airl':
-            reward = (s + eps).log() - (1 - s + eps).log()
-        elif self.args.reward_type == 'gail':
-            reward = (s + eps).log()
-        elif self.args.reward_type == 'raw':
-            reward = d_val
-        else:
-            raise ValueError(f"Unrecognized reward type {self.args.reward_type}")
-        return reward
-
-    def _get_reward(self, step, storage, add_info):
-        masks = storage.masks[step]
-        with torch.no_grad():
-            self.discrim_net.eval()
-            reward = self._compute_discrim_reward(storage, step, add_info)
-
-            if self.args.gail_reward_norm:
-                if self.returns is None:
-                    self.returns = reward.clone()
-
-                self.returns = self.returns * masks * self.args.gamma + reward
-                self.ret_rms.update(self.returns.cpu().numpy())
-
-                return reward / np.sqrt(self.ret_rms.var[0] + 1e-8), {}
-            else:
-                return reward, {}
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        #########################################
-        # Overrides
-
-        #########################################
-        # New args
-        parser.add_argument('--action-input', type=str2bool, default=False)
-        parser.add_argument('--gail-reward-norm', type=str2bool, default=False)
-        parser.add_argument('--gail-state-norm', type=str2bool, default=True)
-        parser.add_argument('--disc-lr', type=float, default=0.0001)
-        parser.add_argument('--disc-grad-pen', type=float, default=0.0)
-        parser.add_argument('--n-gail-epochs', type=int, default=1)
-        parser.add_argument('--reward-type', type=str, default='airl', help="""
-                One of [airl, raw, gail]. Changes the reward computation. Does
-                not change training.
-                """)
-
-    def load_resume(self, checkpointer):
-        super().load_resume(checkpointer)
-        self.opt.load_state_dict(checkpointer.get_key('gail_disc_opt'))
-        self.discrim_net.load_state_dict(checkpointer.get_key('gail_disc'))
-
-    def save(self, checkpointer):
-        super().save(checkpointer)
-        checkpointer.save_key('gail_disc_opt', self.opt.state_dict())
-        checkpointer.save_key('gail_disc', self.discrim_net.state_dict())
diff --git a/rl-toolkit/rlf/algos/il/gan_bc.py b/rl-toolkit/rlf/algos/il/gan_bc.py
index f481018..50c454d 100644
--- a/rl-toolkit/rlf/algos/il/gan_bc.py
+++ b/rl-toolkit/rlf/algos/il/gan_bc.py
@@ -253,5 +253,4 @@ class GANBC(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=False)
         parser.add_argument("--bc-noise", type=float, default=None)
diff --git a/rl-toolkit/rlf/algos/il/ibc.py b/rl-toolkit/rlf/algos/il/ibc.py
index a69d186..efe86dc 100644
--- a/rl-toolkit/rlf/algos/il/ibc.py
+++ b/rl-toolkit/rlf/algos/il/ibc.py
@@ -217,7 +217,6 @@ class IBC(BaseILAlgo):
         #########################################
         # New args
         parser.add_argument("--bc-num-epochs", type=int, default=1)
-        parser.add_argument("--bc-state-norm", type=str2bool, default=True)
         parser.add_argument("--bc-noise", type=float, default=None)
         parser.add_argument("--temperature", type=float, default=1)
         
diff --git a/rl-toolkit/rlf/algos/il/sqil.py b/rl-toolkit/rlf/algos/il/sqil.py
deleted file mode 100644
index a457394..0000000
--- a/rl-toolkit/rlf/algos/il/sqil.py
+++ /dev/null
@@ -1,106 +0,0 @@
-import torch
-from rlf.algos.il.base_il import BaseILAlgo
-from rlf.algos.il.base_irl import BaseIRLAlgo
-from rlf.algos.off_policy.sac import SAC
-from rlf.storage.transition_storage import TransitionStorage
-
-
-class SqilTransitionStorage(TransitionStorage):
-    def __init__(self, obs_space, action_space, capacity, args, il_algo):
-        if args.traj_batch_size != args.batch_size:
-            raise ValueError(
-                """
-                    Must sample an equal amount of expert and agent experience.
-                    """
-            )
-        self.il_algo = il_algo
-        self.expert_batch_iter = None
-        super().__init__(obs_space, action_space, capacity, args)
-
-    def get_next_expert_batch(self):
-        batch = None
-        if self.expert_batch_iter is not None:
-            try:
-                batch = next(self.expert_batch_iter)
-            except StopIteration:
-                pass
-
-        if batch is None:
-            self.expert_batch_iter = iter(self.il_algo.expert_train_loader)
-            batch = next(self.expert_batch_iter)
-        return batch
-
-    def sample_tensors(self, sample_size):
-        (
-            states,
-            next_states,
-            actions,
-            rewards,
-            cur_add,
-            next_add,
-        ) = super().sample_tensors(sample_size)
-        expert_sample = self.get_next_expert_batch()
-
-        expert_states = self._norm_expert_state(expert_sample["state"])
-        expert_next_states = self._norm_expert_state(expert_sample["next_state"])
-        expert_actions = self.il_algo._adjust_action(expert_sample["actions"])
-
-        next_add["masks"] = torch.cat(
-            [
-                next_add["masks"].to(self.args.device),
-                expert_sample["done"].unsqueeze(-1),
-            ],
-            dim=0,
-        )
-        states = torch.cat([states, expert_states], dim=0)
-        next_states = torch.cat([next_states, expert_next_states], dim=0)
-        rewards = torch.cat(
-            [
-                torch.zeros(rewards.shape).to(rewards.device),
-                torch.ones(rewards.shape).to(rewards.device),
-            ],
-            dim=0,
-        )
-        actions = torch.cat([actions, expert_actions], dim=0)
-
-        return states, next_states, actions, rewards, cur_add, next_add
-
-    def _norm_expert_state(self, state):
-        """
-        Applies normalization to the expert state, if we are using
-        normalization.
-        """
-        obsfilt = self.il_algo.get_env_ob_filt()
-        if obsfilt is None:
-            return state
-        state = state.cpu().numpy()
-        state = obsfilt(state, update=False)
-        state = torch.tensor(state).to(self.args.device)
-        return state
-
-
-class SQIL(SAC):
-    def __init__(self):
-        self.il_algo = BaseILAlgo()
-        super().__init__()
-
-    def get_storage_buffer(self, policy, envs, args):
-        return SqilTransitionStorage(
-            policy.obs_space,
-            policy.action_space,
-            args.trans_buffer_size,
-            args,
-            self.il_algo,
-        )
-
-    def init(self, policy, args):
-        super().init(policy, args)
-        self.il_algo.init(policy, args)
-
-    def set_env_ref(self, envs):
-        super().set_env_ref(envs)
-        self.il_algo.set_env_ref(envs)
-
-    def get_add_args(self, parser):
-        super().get_add_args(parser)
-        self.il_algo.get_add_args(parser)
diff --git a/runs/antReach/bc.sh b/runs/antReach/bc.sh
deleted file mode 100755
index b7d4c5a..0000000
--- a/runs/antReach/bc.sh
+++ /dev/null
@@ -1,71 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --seed=1 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=bc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --seed=2 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=bc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --seed=3 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
\ No newline at end of file
diff --git a/runs/antReach/dbc.sh b/runs/antReach/dbc.sh
deleted file mode 100755
index dabf381..0000000
--- a/runs/antReach/dbc.sh
+++ /dev/null
@@ -1,89 +0,0 @@
-python dbc/main.py \
-    --agent_expert_normalization=False \
-    --alg=dbc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=1 \
-    --cuda=True \
-    --ddpm-path=/home/mhhsu/dbc-private/rl-toolkit/dm/trained_models/ant_1000_ddpm_0.0002_1024_20000_sigmoid.pt \
-    --depth=2 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=30000 \
-    --scheduler-type=sigmoid \
-    --seed=1 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --agent-expert-normalization=True \
-    --alg=dbc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/ant_10000_ddpm_0.0002_1024_20000_sigmoid.pt \
-    --depth=2 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=30000 \
-    --scheduler-type=sigmoid \
-    --seed=2 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --agent-expert-normalization=True \
-    --alg=dbc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/ant_10000_ddpm_0.0002_1024_20000_sigmoid.pt \
-    --depth=2 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=30000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.006 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=30000 \
-    --scheduler-type=sigmoid \
-    --seed=3 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
\ No newline at end of file
diff --git a/runs/antReach/dp.sh b/runs/antReach/dp.sh
deleted file mode 100755
index af32354..0000000
--- a/runs/antReach/dp.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=500 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --seed=1 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=dp \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=500 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --seed=2 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=dp \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=500 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --seed=3 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
\ No newline at end of file
diff --git a/runs/antReach/ibc.sh b/runs/antReach/ibc.sh
deleted file mode 100755
index 0f28b40..0000000
--- a/runs/antReach/ibc.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=5e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --seed=1 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=ibc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=5e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --seed=2 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
-
-python dbc/main.py \
-    --alg=ibc \
-    --ant-noise=0.01 \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=AntGoal-v0 \
-    --eval-interval=100000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=5e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --seed=3 \
-    --traj-frac=1 \
-    --traj-load-path=./expert_datasets/ant_10000.pt \
-    --vid-fps=100
\ No newline at end of file
diff --git a/runs/antReach/train_dm.sh b/runs/antReach/train_dm.sh
deleted file mode 100755
index acab9be..0000000
--- a/runs/antReach/train_dm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path ./expert_datasets/ant_10000.pt \
-    --lr 0.0002 \
-    --hidden-dim 1024 \
-    --norm False \
-    --scheduler-type sigmoid \
-    --num-epoch 20000 
\ No newline at end of file
diff --git a/runs/fetchPick/bc.sh b/runs/fetchPick/bc.sh
deleted file mode 100755
index 7fd9d1b..0000000
--- a/runs/fetchPick/bc.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --hidden-dim=750 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --max-grad-norm=-1 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --hidden-dim=750 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --max-grad-norm=-1 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --hidden-dim=750 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --max-grad-norm=-1 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/fetchPick/dbc.sh b/runs/fetchPick/dbc.sh
deleted file mode 100755
index b0155f7..0000000
--- a/runs/fetchPick/dbc.sh
+++ /dev/null
@@ -1,86 +0,0 @@
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.5 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/pick_10000_clip_ddpm_0.001_1024_10000_sigmoid.pt \
-    --depth=2 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=750 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --num-units 1024 \
-    --scheduler-type sigmoid \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.5 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/pick_10000_clip_ddpm_0.001_1024_10000_sigmoid.pt \
-    --depth=2 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=750 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --num-units 1024 \
-    --scheduler-type sigmoid \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.5 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/pick_10000_clip_ddpm_0.001_1024_10000_sigmoid.pt \
-    --depth=2 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=750 \
-    --log-interval=2000 \
-    --lr=0.000005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=100 \
-    --num-units 1024 \
-    --scheduler-type sigmoid \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/fetchPick/dp.sh b/runs/fetchPick/dp.sh
deleted file mode 100755
index 7f6edbe..0000000
--- a/runs/fetchPick/dp.sh
+++ /dev/null
@@ -1,74 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=9000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=3000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=500000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=30 \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=9000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=3000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=500000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=30 \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=9000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=3000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=500000 \
-    --traj-batch-size=128 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --vid-fps=30 \
-    --noise-ratio=1.25 \
-    --goal-noise-ratio=1.25 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/fetchPick/ibc.sh b/runs/fetchPick/ibc.sh
deleted file mode 100755
index 2dd0ac1..0000000
--- a/runs/fetchPick/ibc.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --hidden-dim=1024 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --log-interval=1000 \
-    --lr=5e-06 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=50000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=1
-    
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --hidden-dim=1024 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --log-interval=1000 \
-    --lr=5e-06 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=50000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --hidden-dim=1024 \
-    --env-name=FetchPickAndPlaceDiffHoldout-v0 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --log-interval=1000 \
-    --lr=5e-06 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=50000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/pick_10000_clip.pt \
-    --goal-noise-ratio=1.25 \
-    --noise-ratio=1.25 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/fetchPick/train_dm.sh b/runs/fetchPick/train_dm.sh
deleted file mode 100755
index c6aca32..0000000
--- a/runs/fetchPick/train_dm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path expert_datasets/pick_10000_clip.pt \
-    --lr 0.001 \
-    --hidden-dim 1024 \
-    --norm True \
-    --scheduler-type sigmoid \
-    --num-epoch 10000 
\ No newline at end of file
diff --git a/runs/halfcheetah/bc.sh b/runs/halfcheetah/bc.sh
deleted file mode 100755
index 63d53cf..0000000
--- a/runs/halfcheetah/bc.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/halfcheetah/dbc.sh b/runs/halfcheetah/dbc.sh
deleted file mode 100755
index 9ee11ca..0000000
--- a/runs/halfcheetah/dbc.sh
+++ /dev/null
@@ -1,68 +0,0 @@
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/halfcheetah_5trajs_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --scheduler-type sigmoid \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/halfcheetah_5trajs_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --scheduler-type sigmoid \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/halfcheetah_5trajs_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --hidden-dim 256 \
-    --scheduler-type sigmoid \
-    --seed=3
\ No newline at end of file
diff --git a/runs/halfcheetah/dp.sh b/runs/halfcheetah/dp.sh
deleted file mode 100755
index 8f6d8b2..0000000
--- a/runs/halfcheetah/dp.sh
+++ /dev/null
@@ -1,67 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=2
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=3
\ No newline at end of file
diff --git a/runs/halfcheetah/ibc.sh b/runs/halfcheetah/ibc.sh
deleted file mode 100755
index fd1e18b..0000000
--- a/runs/halfcheetah/ibc.sh
+++ /dev/null
@@ -1,65 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=HalfCheetah-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/halfcheetah_5trajs_processed.pt \
-    --vid-fps=100 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/halfcheetah/train_dm.sh b/runs/halfcheetah/train_dm.sh
deleted file mode 100755
index 86613d9..0000000
--- a/runs/halfcheetah/train_dm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path ./expert_datasets/halfcheetah_5traj_processed.pt \
-    --lr 0.0002 \
-    --hidden-dim 1024 \
-    --norm True \
-    --scheduler-type sigmoid \
-    --num-epoch 8000 
\ No newline at end of file
diff --git a/runs/hand/bc.sh b/runs/hand/bc.sh
deleted file mode 100755
index 439be97..0000000
--- a/runs/hand/bc.sh
+++ /dev/null
@@ -1,65 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-04 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-04 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-04 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=1 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --seed=3
diff --git a/runs/hand/dbc.sh b/runs/hand/dbc.sh
deleted file mode 100755
index 8c5e86d..0000000
--- a/runs/hand/dbc.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=10 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/hand_10000_v2_ddpm_3e-05_2048_10000_sigmoid_norm.pt \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=512 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --scheduler-type sigmoid \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=10 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/hand_10000_v2_ddpm_3e-05_2048_10000_sigmoid_norm.pt \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=512 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --scheduler-type sigmoid \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=5000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=10 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/hand_10000_v2_ddpm_3e-05_2048_10000_sigmoid_norm.pt \
-    --depth=2 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=512 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=1 \
-    --prefix=dbc \
-    --save-interval=10000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --vid-fps=100 \
-    --scheduler-type sigmoid \
-    --seed=3
\ No newline at end of file
diff --git a/runs/hand/dp.sh b/runs/hand/dp.sh
deleted file mode 100755
index d193255..0000000
--- a/runs/hand/dp.sh
+++ /dev/null
@@ -1,68 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=3 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --hidden-dim 2100 \
-    --dp-scheduler-type sigmoid \
-    --vid-fps=30 \
-    --depth 4 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=3 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --hidden-dim 2100 \
-    --dp-scheduler-type sigmoid \
-    --vid-fps=30 \
-    --depth 4 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=30000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --log-interval=2000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=3 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --hidden-dim 2100 \
-    --dp-scheduler-type sigmoid \
-    --vid-fps=30 \
-    --depth 4 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/hand/ibc.sh b/runs/hand/ibc.sh
deleted file mode 100755
index 6a65255..0000000
--- a/runs/hand/ibc.sh
+++ /dev/null
@@ -1,71 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --seed=1
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --seed=2
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=15000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=CustomHandManipulateBlockRotateZ-v2 \
-    --eval-interval=3000 \
-    --eval-num-processes=10 \
-    --hidden-dim=512 \
-    --log-interval=1000 \
-    --lr=1e-05 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-process=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --stochastic-optimizer-train_samples=256 \
-    --traj-batch-size=512 \
-    --traj-load-path=./expert_datasets/hand_10000_v2.pt \
-    --seed=3
\ No newline at end of file
diff --git a/runs/hand/train_dm.sh b/runs/hand/train_dm.sh
deleted file mode 100755
index e9c324d..0000000
--- a/runs/hand/train_dm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path ./expert_datasets/hand_10000_v2.pt \
-    --lr 0.00003 \
-    --hidden-dim 2048 \
-    --norm True \
-    --scheduler-type sigmoid \
-    --num-epoch 10000 
\ No newline at end of file
diff --git a/runs/maze/bc.sh b/runs/maze/bc.sh
deleted file mode 100755
index 58883d5..0000000
--- a/runs/maze/bc.sh
+++ /dev/null
@@ -1,92 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=3 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --log-diff-loss=True \
-    --prefix=bc-100 \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt \
-    --bc-state-norm=True \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --seed=1
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=3 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --log-diff-loss=True \
-    --prefix=bc-100 \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt \
-    --bc-state-norm=True \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --seed=2
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=1 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=100 \
-    --num-render=3 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --log-diff-loss=True \
-    --prefix=bc-100 \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt \
-    --bc-state-norm=True \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --seed=3
\ No newline at end of file
diff --git a/runs/maze/dbc.sh b/runs/maze/dbc.sh
deleted file mode 100755
index 347113a..0000000
--- a/runs/maze/dbc.sh
+++ /dev/null
@@ -1,86 +0,0 @@
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=30 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid.pt \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=10 \
-    --hidden-dim=256 \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --traj-batch-size 128 \
-    --scheduler-type sigmoid \
-    --prefix=dbc \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=30 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=10 \
-    --hidden-dim=256 \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --traj-batch-size 128 \
-    --scheduler-type sigmoid \
-    --prefix=dbc \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=2000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=30 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --ddpm-path=./rl-toolkit/dm/trained_models/maze2d_100_ddpm_0.0001_128_sigmoid_norm.pt \
-    --depth=3 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=2000 \
-    --eval-num-processes=10 \
-    --hidden-dim=256 \
-    --il-in-action-norm=True \
-    --il-out-action-norm=True \
-    --log-interval=200 \
-    --lr=0.00005 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --save-interval=20000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --traj-batch-size 128 \
-    --scheduler-type sigmoid \
-    --prefix=dbc \
-    --seed=3
\ No newline at end of file
diff --git a/runs/maze/dp.sh b/runs/maze/dp.sh
deleted file mode 100755
index b65108b..0000000
--- a/runs/maze/dp.sh
+++ /dev/null
@@ -1,71 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=55000 \
-    --eval-num-processes=20 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=5000 \
-    --lr=0.0002 \
-    --normalize-env=False \
-    --num-eval=5 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=30 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=55000 \
-    --eval-num-processes=20 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=5000 \
-    --lr=0.0002 \
-    --normalize-env=False \
-    --num-eval=5 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=30 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=20000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=55000 \
-    --eval-num-processes=20 \
-    --hidden-dim=256 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=5000 \
-    --lr=0.0002 \
-    --normalize-env=False \
-    --num-eval=5 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=30 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/maze/ibc.sh b/runs/maze/ibc.sh
deleted file mode 100755
index ebb4481..0000000
--- a/runs/maze/ibc.sh
+++ /dev/null
@@ -1,77 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=1 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=15000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=1000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=1 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=15000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=1000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=False \
-    --clip-actions=True \
-    --coeff=0 \
-    --coeff-bc=1 \
-    --cuda=True \
-    --depth=1 \
-    --env-name=maze2d-medium-v2 \
-    --eval-interval=15000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --il-in-action-norm=False \
-    --il-out-action-norm=False \
-    --log-interval=1000 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=500000 \
-    --traj-load-path=./expert_datasets/maze2d_100.pt \
-    --vid-fps=60 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/maze/train_dm.sh b/runs/maze/train_dm.sh
deleted file mode 100755
index 0ca9ef0..0000000
--- a/runs/maze/train_dm.sh
+++ /dev/null
@@ -1,6 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path ./expert_datasets/maze2d_100.pt \
-    --lr 0.0001 \
-    --hidden-dim 128 \
-    --norm False \
-    --scheduler-type sigmoid 
\ No newline at end of file
diff --git a/runs/walker/bc.sh b/runs/walker/bc.sh
deleted file mode 100755
index 17816d1..0000000
--- a/runs/walker/bc.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=bc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=bc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/walker/dbc.sh b/runs/walker/dbc.sh
deleted file mode 100755
index 3a47845..0000000
--- a/runs/walker/dbc.sh
+++ /dev/null
@@ -1,65 +0,0 @@
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/walker_5traj_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/walker_5traj_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dbc \
-    --bc-num-epochs=1000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --coeff=0.2 \
-    --cuda=True \
-    --ddpm-path=rl-toolkit/dm/trained_models/walker_5traj_processed_ddpm_0.0002_1024_8000_sigmoid_norm.pt \
-    --env-name=Walker2d-v3 \
-    --eval-interval=1000 \
-    --eval-num-processes=10 \
-    --log-interval=200 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=10 \
-    --num-render=0 \
-    --prefix=dbc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/walker/dp.sh b/runs/walker/dp.sh
deleted file mode 100755
index 51b673b..0000000
--- a/runs/walker/dp.sh
+++ /dev/null
@@ -1,68 +0,0 @@
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=1
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=2
-
-python dbc/main.py \
-    --alg=dp \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=4 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=20000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1200 \
-    --log-interval=500 \
-    --lr=0.0001 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=dp \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --dp-scheduler-type sigmoid \
-    --seed=3
\ No newline at end of file
diff --git a/runs/walker/ibc.sh b/runs/walker/ibc.sh
deleted file mode 100755
index c860597..0000000
--- a/runs/walker/ibc.sh
+++ /dev/null
@@ -1,65 +0,0 @@
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --log-interval=200 \
-    --lr=0.00008 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=1
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --log-interval=200 \
-    --lr=0.00008 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=2
-
-python dbc/main.py \
-    --alg=ibc \
-    --bc-num-epochs=10000 \
-    --bc-state-norm=True \
-    --clip-actions=True \
-    --cuda=True \
-    --depth=1 \
-    --env-name=Walker2d-v3 \
-    --eval-interval=4000 \
-    --eval-num-processes=10 \
-    --hidden-dim=1024 \
-    --log-interval=200 \
-    --lr=0.00008 \
-    --normalize-env=False \
-    --num-eval=3 \
-    --num-render=0 \
-    --prefix=ibc \
-    --save-interval=100000 \
-    --traj-load-path=./expert_datasets/walker_5traj_processed.pt \
-    --vid-fps=100 \
-    --seed=3
\ No newline at end of file
diff --git a/runs/walker/train_dm.sh b/runs/walker/train_dm.sh
deleted file mode 100755
index 560df5f..0000000
--- a/runs/walker/train_dm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-python rl-toolkit/dm/ddpm.py \
-    --traj-load-path ./expert_datasets/walker_5traj_processed.pt \
-    --lr 0.0002 \
-    --hidden-dim 1024 \
-    --norm True \
-    --scheduler-type sigmoid \
-    --num-epoch 8000 
\ No newline at end of file
